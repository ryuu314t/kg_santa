{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本変数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_flg=0 #サンプリング有無をコントロール\n",
    "submit_flg=1 #保存するかをコントロール（サンプリングしない時のみ）\n",
    "\n",
    "SEED=12345\n",
    "sample_num=1000\n",
    "fold_num=5\n",
    "\n",
    "#train関連\n",
    "train_dir='../../01_input/train.csv'\n",
    "train_drop_col=['ID_code', 'target']\n",
    "train_label='target'\n",
    "\n",
    "#test関連\n",
    "test_dir='../../01_input/test.csv'\n",
    "test_drop_col=['ID_code']\n",
    "\n",
    "#結果ファイル関連　nameは自分の名前に変更する\n",
    "train_preds_dir='../../03_predict_train/name_200_LightGBM_train.csv'\n",
    "test_preds_dir='../../04_predict_test/name_200_LightGBM_submission.csv'\n",
    "save_col_name='oof_xgb'\n",
    "\n",
    "sample_submission_dir='../../01_input/sample_submission.csv'\n",
    "submission_target_col_name='target'\n",
    "submission_id_col_name='ID_code'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ロード\n",
    "train_df=pd.read_csv(train_dir)\n",
    "test_df=pd.read_csv(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#サンプリング\n",
    "if sampling_flg ==1:\n",
    "    train_df=train_df.sample(n=sample_num,random_state=SEED)\n",
    "    test_df=test_df.sample(n=sample_num,random_state=SEED)\n",
    "    train_df=train_df.reset_index()\n",
    "    test_df=test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x,y作成\n",
    "x_train=train_df.drop(train_drop_col,axis=1)\n",
    "y_train=train_df[train_label]\n",
    "x_test=test_df.drop(test_drop_col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_col=x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=KMeans(n_clusters=10, init='k-means++', n_init=10, max_iter=300,\n",
    "                               tol=0.0001,precompute_distances='auto', verbose=0,\n",
    "                               random_state=11111, copy_x=True, n_jobs=1)\n",
    "clf.fit(x_train)\n",
    "pred=clf.predict(x_test)\n",
    "x_test['kmeans']=pred\n",
    "pred=clf.predict(x_train)\n",
    "x_train['kmeans']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "      <th>kmeans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>8.8100</td>\n",
       "      <td>...</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>5.9739</td>\n",
       "      <td>...</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>8.3442</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>7.4578</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>7.1437</td>\n",
       "      <td>...</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.9862</td>\n",
       "      <td>-2.2913</td>\n",
       "      <td>8.6058</td>\n",
       "      <td>7.0685</td>\n",
       "      <td>14.2465</td>\n",
       "      <td>-8.6761</td>\n",
       "      <td>4.2467</td>\n",
       "      <td>14.7632</td>\n",
       "      <td>1.8790</td>\n",
       "      <td>7.2842</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1178</td>\n",
       "      <td>-0.4249</td>\n",
       "      <td>8.8781</td>\n",
       "      <td>14.9438</td>\n",
       "      <td>-2.2151</td>\n",
       "      <td>-6.0233</td>\n",
       "      <td>9.8117</td>\n",
       "      <td>17.1127</td>\n",
       "      <td>10.8240</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.4624</td>\n",
       "      <td>-6.1065</td>\n",
       "      <td>7.3603</td>\n",
       "      <td>8.2627</td>\n",
       "      <td>12.0104</td>\n",
       "      <td>-7.2073</td>\n",
       "      <td>4.1670</td>\n",
       "      <td>13.0809</td>\n",
       "      <td>-4.3004</td>\n",
       "      <td>6.3181</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8661</td>\n",
       "      <td>4.0971</td>\n",
       "      <td>8.8484</td>\n",
       "      <td>17.5010</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>7.7443</td>\n",
       "      <td>9.1509</td>\n",
       "      <td>18.4736</td>\n",
       "      <td>5.1499</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.3035</td>\n",
       "      <td>-2.4212</td>\n",
       "      <td>13.3989</td>\n",
       "      <td>8.3998</td>\n",
       "      <td>11.0777</td>\n",
       "      <td>9.6449</td>\n",
       "      <td>5.9596</td>\n",
       "      <td>17.8477</td>\n",
       "      <td>-4.8068</td>\n",
       "      <td>7.4643</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4214</td>\n",
       "      <td>0.9303</td>\n",
       "      <td>1.4994</td>\n",
       "      <td>15.2648</td>\n",
       "      <td>-1.7931</td>\n",
       "      <td>6.5316</td>\n",
       "      <td>10.4855</td>\n",
       "      <td>23.4631</td>\n",
       "      <td>0.7283</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.9856</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>13.7161</td>\n",
       "      <td>4.7749</td>\n",
       "      <td>8.6784</td>\n",
       "      <td>-13.7607</td>\n",
       "      <td>4.3386</td>\n",
       "      <td>14.5843</td>\n",
       "      <td>2.5883</td>\n",
       "      <td>7.2215</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8754</td>\n",
       "      <td>2.4698</td>\n",
       "      <td>-0.0362</td>\n",
       "      <td>16.7144</td>\n",
       "      <td>0.1221</td>\n",
       "      <td>-1.4328</td>\n",
       "      <td>9.9207</td>\n",
       "      <td>16.9865</td>\n",
       "      <td>-3.3304</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.3811</td>\n",
       "      <td>-6.9348</td>\n",
       "      <td>14.6690</td>\n",
       "      <td>9.0941</td>\n",
       "      <td>11.9058</td>\n",
       "      <td>-10.8018</td>\n",
       "      <td>3.4508</td>\n",
       "      <td>20.2816</td>\n",
       "      <td>-1.4112</td>\n",
       "      <td>6.7401</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0723</td>\n",
       "      <td>0.8907</td>\n",
       "      <td>4.7680</td>\n",
       "      <td>15.1425</td>\n",
       "      <td>0.6075</td>\n",
       "      <td>-4.4447</td>\n",
       "      <td>9.5788</td>\n",
       "      <td>15.8146</td>\n",
       "      <td>9.3457</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.3431</td>\n",
       "      <td>-4.1427</td>\n",
       "      <td>9.1985</td>\n",
       "      <td>9.8229</td>\n",
       "      <td>11.2494</td>\n",
       "      <td>2.9678</td>\n",
       "      <td>5.5184</td>\n",
       "      <td>15.6290</td>\n",
       "      <td>2.8032</td>\n",
       "      <td>8.9513</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7916</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>-2.1353</td>\n",
       "      <td>23.2482</td>\n",
       "      <td>0.8738</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>10.1361</td>\n",
       "      <td>12.1140</td>\n",
       "      <td>-2.4978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.6137</td>\n",
       "      <td>-2.1898</td>\n",
       "      <td>8.9090</td>\n",
       "      <td>3.8014</td>\n",
       "      <td>13.8602</td>\n",
       "      <td>-5.9802</td>\n",
       "      <td>5.5515</td>\n",
       "      <td>15.4716</td>\n",
       "      <td>-0.1714</td>\n",
       "      <td>7.6178</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0625</td>\n",
       "      <td>-0.1537</td>\n",
       "      <td>7.9787</td>\n",
       "      <td>18.4518</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>-7.8212</td>\n",
       "      <td>9.2355</td>\n",
       "      <td>15.0721</td>\n",
       "      <td>-7.3475</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.7465</td>\n",
       "      <td>-4.9467</td>\n",
       "      <td>15.5490</td>\n",
       "      <td>6.4580</td>\n",
       "      <td>13.7572</td>\n",
       "      <td>-25.5371</td>\n",
       "      <td>4.4893</td>\n",
       "      <td>15.1682</td>\n",
       "      <td>3.1754</td>\n",
       "      <td>7.5722</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3879</td>\n",
       "      <td>1.9888</td>\n",
       "      <td>-0.6670</td>\n",
       "      <td>17.6003</td>\n",
       "      <td>-0.7705</td>\n",
       "      <td>6.5424</td>\n",
       "      <td>8.9599</td>\n",
       "      <td>16.9317</td>\n",
       "      <td>-14.0779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.7836</td>\n",
       "      <td>1.9979</td>\n",
       "      <td>10.3347</td>\n",
       "      <td>7.8857</td>\n",
       "      <td>13.1020</td>\n",
       "      <td>5.0167</td>\n",
       "      <td>4.9548</td>\n",
       "      <td>23.6527</td>\n",
       "      <td>3.5911</td>\n",
       "      <td>5.8546</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3561</td>\n",
       "      <td>3.2395</td>\n",
       "      <td>3.9152</td>\n",
       "      <td>16.6549</td>\n",
       "      <td>0.4106</td>\n",
       "      <td>-2.1647</td>\n",
       "      <td>10.0379</td>\n",
       "      <td>20.5904</td>\n",
       "      <td>6.0166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.0360</td>\n",
       "      <td>1.6797</td>\n",
       "      <td>9.3865</td>\n",
       "      <td>3.2605</td>\n",
       "      <td>10.7569</td>\n",
       "      <td>-8.0802</td>\n",
       "      <td>4.7885</td>\n",
       "      <td>15.0583</td>\n",
       "      <td>0.6459</td>\n",
       "      <td>4.8661</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0835</td>\n",
       "      <td>2.7034</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>18.8216</td>\n",
       "      <td>-0.8522</td>\n",
       "      <td>10.3456</td>\n",
       "      <td>8.2077</td>\n",
       "      <td>17.6097</td>\n",
       "      <td>-0.9141</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.8595</td>\n",
       "      <td>-4.5378</td>\n",
       "      <td>13.6483</td>\n",
       "      <td>5.6480</td>\n",
       "      <td>9.9144</td>\n",
       "      <td>1.5190</td>\n",
       "      <td>5.0358</td>\n",
       "      <td>13.4524</td>\n",
       "      <td>-2.5419</td>\n",
       "      <td>9.4450</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8526</td>\n",
       "      <td>4.8517</td>\n",
       "      <td>2.5020</td>\n",
       "      <td>22.8224</td>\n",
       "      <td>-0.9325</td>\n",
       "      <td>8.6849</td>\n",
       "      <td>10.2848</td>\n",
       "      <td>17.4932</td>\n",
       "      <td>6.0800</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.1732</td>\n",
       "      <td>-5.1490</td>\n",
       "      <td>9.7591</td>\n",
       "      <td>3.7316</td>\n",
       "      <td>10.3700</td>\n",
       "      <td>-21.9202</td>\n",
       "      <td>7.7130</td>\n",
       "      <td>18.8749</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>7.8453</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9058</td>\n",
       "      <td>1.3140</td>\n",
       "      <td>4.8961</td>\n",
       "      <td>20.1087</td>\n",
       "      <td>1.1051</td>\n",
       "      <td>7.7184</td>\n",
       "      <td>9.3406</td>\n",
       "      <td>21.1746</td>\n",
       "      <td>-2.0098</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.0936</td>\n",
       "      <td>-8.7414</td>\n",
       "      <td>17.1160</td>\n",
       "      <td>6.0126</td>\n",
       "      <td>9.2144</td>\n",
       "      <td>-3.6761</td>\n",
       "      <td>4.6477</td>\n",
       "      <td>20.1053</td>\n",
       "      <td>1.7687</td>\n",
       "      <td>7.9974</td>\n",
       "      <td>...</td>\n",
       "      <td>7.7219</td>\n",
       "      <td>0.4559</td>\n",
       "      <td>2.7977</td>\n",
       "      <td>21.4898</td>\n",
       "      <td>-0.2385</td>\n",
       "      <td>8.1878</td>\n",
       "      <td>9.3114</td>\n",
       "      <td>23.0545</td>\n",
       "      <td>-2.3171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15.7875</td>\n",
       "      <td>0.1671</td>\n",
       "      <td>10.7782</td>\n",
       "      <td>3.8521</td>\n",
       "      <td>9.1190</td>\n",
       "      <td>11.0196</td>\n",
       "      <td>6.1113</td>\n",
       "      <td>18.4368</td>\n",
       "      <td>-1.0728</td>\n",
       "      <td>7.0586</td>\n",
       "      <td>...</td>\n",
       "      <td>13.1753</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>6.6279</td>\n",
       "      <td>19.2890</td>\n",
       "      <td>-0.0982</td>\n",
       "      <td>7.5500</td>\n",
       "      <td>9.3848</td>\n",
       "      <td>14.4007</td>\n",
       "      <td>11.2567</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.3874</td>\n",
       "      <td>1.0716</td>\n",
       "      <td>8.8767</td>\n",
       "      <td>7.8374</td>\n",
       "      <td>11.6404</td>\n",
       "      <td>6.2512</td>\n",
       "      <td>4.8837</td>\n",
       "      <td>18.2178</td>\n",
       "      <td>4.3871</td>\n",
       "      <td>9.5648</td>\n",
       "      <td>...</td>\n",
       "      <td>11.8869</td>\n",
       "      <td>1.6137</td>\n",
       "      <td>4.7306</td>\n",
       "      <td>15.0826</td>\n",
       "      <td>-0.2445</td>\n",
       "      <td>2.1377</td>\n",
       "      <td>8.1116</td>\n",
       "      <td>20.5129</td>\n",
       "      <td>5.4945</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.0259</td>\n",
       "      <td>-4.6740</td>\n",
       "      <td>8.6431</td>\n",
       "      <td>2.2198</td>\n",
       "      <td>11.4555</td>\n",
       "      <td>-14.0227</td>\n",
       "      <td>6.9192</td>\n",
       "      <td>17.8559</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>6.5548</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9088</td>\n",
       "      <td>-0.2736</td>\n",
       "      <td>5.4434</td>\n",
       "      <td>19.9933</td>\n",
       "      <td>-0.6241</td>\n",
       "      <td>11.7708</td>\n",
       "      <td>7.8690</td>\n",
       "      <td>18.1980</td>\n",
       "      <td>-3.7655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14.3356</td>\n",
       "      <td>0.2317</td>\n",
       "      <td>9.5604</td>\n",
       "      <td>5.7603</td>\n",
       "      <td>10.3184</td>\n",
       "      <td>-6.4721</td>\n",
       "      <td>4.6898</td>\n",
       "      <td>13.7783</td>\n",
       "      <td>1.8342</td>\n",
       "      <td>9.2284</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8724</td>\n",
       "      <td>1.5538</td>\n",
       "      <td>4.6148</td>\n",
       "      <td>13.1176</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>9.3434</td>\n",
       "      <td>10.7644</td>\n",
       "      <td>18.0992</td>\n",
       "      <td>-7.6905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.4255</td>\n",
       "      <td>-6.1758</td>\n",
       "      <td>12.4846</td>\n",
       "      <td>7.9845</td>\n",
       "      <td>9.7032</td>\n",
       "      <td>-14.5969</td>\n",
       "      <td>4.4173</td>\n",
       "      <td>19.3606</td>\n",
       "      <td>-0.5899</td>\n",
       "      <td>6.9213</td>\n",
       "      <td>...</td>\n",
       "      <td>4.7884</td>\n",
       "      <td>5.0819</td>\n",
       "      <td>-5.1081</td>\n",
       "      <td>16.3797</td>\n",
       "      <td>-0.1113</td>\n",
       "      <td>4.3362</td>\n",
       "      <td>9.0132</td>\n",
       "      <td>16.9635</td>\n",
       "      <td>3.2523</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.3322</td>\n",
       "      <td>-6.3835</td>\n",
       "      <td>7.2471</td>\n",
       "      <td>5.0403</td>\n",
       "      <td>10.0875</td>\n",
       "      <td>-1.5252</td>\n",
       "      <td>5.8230</td>\n",
       "      <td>17.9494</td>\n",
       "      <td>-3.8454</td>\n",
       "      <td>6.2356</td>\n",
       "      <td>...</td>\n",
       "      <td>11.1389</td>\n",
       "      <td>1.4025</td>\n",
       "      <td>-0.4134</td>\n",
       "      <td>16.9806</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>13.9872</td>\n",
       "      <td>8.2489</td>\n",
       "      <td>22.4405</td>\n",
       "      <td>-5.4524</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.1844</td>\n",
       "      <td>-9.1044</td>\n",
       "      <td>9.7453</td>\n",
       "      <td>9.2638</td>\n",
       "      <td>9.3302</td>\n",
       "      <td>2.6818</td>\n",
       "      <td>5.4711</td>\n",
       "      <td>18.5414</td>\n",
       "      <td>2.2065</td>\n",
       "      <td>8.3338</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4174</td>\n",
       "      <td>0.7743</td>\n",
       "      <td>4.5556</td>\n",
       "      <td>17.7824</td>\n",
       "      <td>-0.2536</td>\n",
       "      <td>7.3867</td>\n",
       "      <td>7.9695</td>\n",
       "      <td>12.6078</td>\n",
       "      <td>7.3475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0029</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>7.5335</td>\n",
       "      <td>6.9343</td>\n",
       "      <td>11.6866</td>\n",
       "      <td>-6.5147</td>\n",
       "      <td>6.7327</td>\n",
       "      <td>19.8941</td>\n",
       "      <td>-6.6497</td>\n",
       "      <td>8.0114</td>\n",
       "      <td>...</td>\n",
       "      <td>5.1404</td>\n",
       "      <td>1.2851</td>\n",
       "      <td>2.2366</td>\n",
       "      <td>23.5212</td>\n",
       "      <td>1.3798</td>\n",
       "      <td>7.6392</td>\n",
       "      <td>6.9239</td>\n",
       "      <td>18.6460</td>\n",
       "      <td>-17.7609</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.9056</td>\n",
       "      <td>-4.8626</td>\n",
       "      <td>11.8932</td>\n",
       "      <td>5.3693</td>\n",
       "      <td>11.2551</td>\n",
       "      <td>-18.9716</td>\n",
       "      <td>5.5991</td>\n",
       "      <td>18.9809</td>\n",
       "      <td>5.5612</td>\n",
       "      <td>7.8337</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7125</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>6.0765</td>\n",
       "      <td>17.6562</td>\n",
       "      <td>2.1001</td>\n",
       "      <td>-1.5770</td>\n",
       "      <td>8.0712</td>\n",
       "      <td>16.6316</td>\n",
       "      <td>7.8962</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.7562</td>\n",
       "      <td>-3.0647</td>\n",
       "      <td>11.7990</td>\n",
       "      <td>9.2162</td>\n",
       "      <td>10.9847</td>\n",
       "      <td>-22.4902</td>\n",
       "      <td>4.2991</td>\n",
       "      <td>13.9800</td>\n",
       "      <td>3.3233</td>\n",
       "      <td>7.7593</td>\n",
       "      <td>...</td>\n",
       "      <td>4.7817</td>\n",
       "      <td>1.3885</td>\n",
       "      <td>-1.3448</td>\n",
       "      <td>23.1983</td>\n",
       "      <td>-1.5301</td>\n",
       "      <td>3.9577</td>\n",
       "      <td>8.7277</td>\n",
       "      <td>16.8255</td>\n",
       "      <td>-18.6596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.7243</td>\n",
       "      <td>-1.5151</td>\n",
       "      <td>11.5582</td>\n",
       "      <td>5.7360</td>\n",
       "      <td>12.1907</td>\n",
       "      <td>6.9664</td>\n",
       "      <td>4.4125</td>\n",
       "      <td>17.4770</td>\n",
       "      <td>-3.9683</td>\n",
       "      <td>7.5912</td>\n",
       "      <td>...</td>\n",
       "      <td>11.2645</td>\n",
       "      <td>3.1891</td>\n",
       "      <td>-0.0828</td>\n",
       "      <td>18.2102</td>\n",
       "      <td>1.2457</td>\n",
       "      <td>1.9391</td>\n",
       "      <td>8.6899</td>\n",
       "      <td>14.5739</td>\n",
       "      <td>16.2884</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.2430</td>\n",
       "      <td>1.2738</td>\n",
       "      <td>10.4245</td>\n",
       "      <td>3.1863</td>\n",
       "      <td>11.4951</td>\n",
       "      <td>-1.4755</td>\n",
       "      <td>5.1005</td>\n",
       "      <td>17.1687</td>\n",
       "      <td>1.7115</td>\n",
       "      <td>9.1463</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8381</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>2.2038</td>\n",
       "      <td>20.3427</td>\n",
       "      <td>1.1092</td>\n",
       "      <td>15.0418</td>\n",
       "      <td>8.4667</td>\n",
       "      <td>15.1478</td>\n",
       "      <td>6.2962</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199970</th>\n",
       "      <td>12.7260</td>\n",
       "      <td>-1.6706</td>\n",
       "      <td>12.3598</td>\n",
       "      <td>9.1114</td>\n",
       "      <td>10.1868</td>\n",
       "      <td>-9.5857</td>\n",
       "      <td>5.3494</td>\n",
       "      <td>23.6362</td>\n",
       "      <td>2.0626</td>\n",
       "      <td>6.2033</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5029</td>\n",
       "      <td>-0.1536</td>\n",
       "      <td>1.9221</td>\n",
       "      <td>11.4131</td>\n",
       "      <td>0.6650</td>\n",
       "      <td>-5.2401</td>\n",
       "      <td>9.1059</td>\n",
       "      <td>19.3888</td>\n",
       "      <td>-7.6292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199971</th>\n",
       "      <td>9.4700</td>\n",
       "      <td>-6.7655</td>\n",
       "      <td>12.6591</td>\n",
       "      <td>9.1842</td>\n",
       "      <td>11.8260</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>5.0633</td>\n",
       "      <td>20.7034</td>\n",
       "      <td>2.4171</td>\n",
       "      <td>8.2646</td>\n",
       "      <td>...</td>\n",
       "      <td>11.5775</td>\n",
       "      <td>1.4309</td>\n",
       "      <td>8.0710</td>\n",
       "      <td>15.0869</td>\n",
       "      <td>-0.9276</td>\n",
       "      <td>-0.5783</td>\n",
       "      <td>8.2742</td>\n",
       "      <td>18.2273</td>\n",
       "      <td>-1.0124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199972</th>\n",
       "      <td>13.3243</td>\n",
       "      <td>1.0870</td>\n",
       "      <td>8.4555</td>\n",
       "      <td>3.6929</td>\n",
       "      <td>11.2423</td>\n",
       "      <td>1.3986</td>\n",
       "      <td>4.4765</td>\n",
       "      <td>19.1021</td>\n",
       "      <td>-2.6573</td>\n",
       "      <td>8.6612</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2868</td>\n",
       "      <td>3.1847</td>\n",
       "      <td>8.8422</td>\n",
       "      <td>15.4298</td>\n",
       "      <td>-1.3178</td>\n",
       "      <td>-9.4033</td>\n",
       "      <td>9.7116</td>\n",
       "      <td>15.5697</td>\n",
       "      <td>-0.7362</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199973</th>\n",
       "      <td>14.2830</td>\n",
       "      <td>-1.8421</td>\n",
       "      <td>11.3664</td>\n",
       "      <td>8.5772</td>\n",
       "      <td>8.8645</td>\n",
       "      <td>-13.8986</td>\n",
       "      <td>4.1603</td>\n",
       "      <td>19.4591</td>\n",
       "      <td>5.6445</td>\n",
       "      <td>9.2011</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2002</td>\n",
       "      <td>3.3686</td>\n",
       "      <td>6.2250</td>\n",
       "      <td>21.0475</td>\n",
       "      <td>-2.0668</td>\n",
       "      <td>0.7093</td>\n",
       "      <td>7.8924</td>\n",
       "      <td>16.0279</td>\n",
       "      <td>7.8535</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199974</th>\n",
       "      <td>4.5171</td>\n",
       "      <td>-5.2068</td>\n",
       "      <td>7.6007</td>\n",
       "      <td>8.1426</td>\n",
       "      <td>10.4433</td>\n",
       "      <td>-17.2322</td>\n",
       "      <td>4.4205</td>\n",
       "      <td>20.3407</td>\n",
       "      <td>-1.0196</td>\n",
       "      <td>5.6569</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4422</td>\n",
       "      <td>3.0645</td>\n",
       "      <td>1.2469</td>\n",
       "      <td>14.8673</td>\n",
       "      <td>-0.4562</td>\n",
       "      <td>13.9675</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>14.3127</td>\n",
       "      <td>-1.2940</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199975</th>\n",
       "      <td>13.4796</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>10.9653</td>\n",
       "      <td>9.1581</td>\n",
       "      <td>13.2959</td>\n",
       "      <td>-3.0995</td>\n",
       "      <td>5.1483</td>\n",
       "      <td>20.9766</td>\n",
       "      <td>1.2932</td>\n",
       "      <td>7.6743</td>\n",
       "      <td>...</td>\n",
       "      <td>4.1240</td>\n",
       "      <td>2.5431</td>\n",
       "      <td>6.2910</td>\n",
       "      <td>14.6021</td>\n",
       "      <td>-3.0392</td>\n",
       "      <td>4.8943</td>\n",
       "      <td>8.7922</td>\n",
       "      <td>19.6428</td>\n",
       "      <td>-24.7859</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199976</th>\n",
       "      <td>12.6337</td>\n",
       "      <td>-6.9793</td>\n",
       "      <td>9.8703</td>\n",
       "      <td>9.9180</td>\n",
       "      <td>10.8092</td>\n",
       "      <td>2.5809</td>\n",
       "      <td>6.7764</td>\n",
       "      <td>18.3443</td>\n",
       "      <td>4.1498</td>\n",
       "      <td>7.8825</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0722</td>\n",
       "      <td>1.2368</td>\n",
       "      <td>0.6363</td>\n",
       "      <td>17.7793</td>\n",
       "      <td>1.1790</td>\n",
       "      <td>-2.0814</td>\n",
       "      <td>10.6610</td>\n",
       "      <td>11.5385</td>\n",
       "      <td>-23.6067</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199977</th>\n",
       "      <td>10.8078</td>\n",
       "      <td>-4.6108</td>\n",
       "      <td>9.0021</td>\n",
       "      <td>9.8910</td>\n",
       "      <td>12.4514</td>\n",
       "      <td>-3.7566</td>\n",
       "      <td>4.2958</td>\n",
       "      <td>19.9677</td>\n",
       "      <td>0.8806</td>\n",
       "      <td>8.2828</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9763</td>\n",
       "      <td>-0.6260</td>\n",
       "      <td>-0.8544</td>\n",
       "      <td>21.5958</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>7.5404</td>\n",
       "      <td>10.0748</td>\n",
       "      <td>16.0592</td>\n",
       "      <td>-14.0012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199978</th>\n",
       "      <td>9.9317</td>\n",
       "      <td>-2.2815</td>\n",
       "      <td>11.1707</td>\n",
       "      <td>5.6826</td>\n",
       "      <td>12.7396</td>\n",
       "      <td>-4.0659</td>\n",
       "      <td>6.2569</td>\n",
       "      <td>12.7697</td>\n",
       "      <td>-2.1645</td>\n",
       "      <td>8.9019</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5345</td>\n",
       "      <td>1.4136</td>\n",
       "      <td>7.5265</td>\n",
       "      <td>20.9985</td>\n",
       "      <td>1.7101</td>\n",
       "      <td>7.9362</td>\n",
       "      <td>7.7696</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-23.5055</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199979</th>\n",
       "      <td>10.5933</td>\n",
       "      <td>-1.2672</td>\n",
       "      <td>13.6817</td>\n",
       "      <td>6.3789</td>\n",
       "      <td>12.8649</td>\n",
       "      <td>-5.4964</td>\n",
       "      <td>6.4800</td>\n",
       "      <td>13.5986</td>\n",
       "      <td>4.0315</td>\n",
       "      <td>8.8308</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5479</td>\n",
       "      <td>1.1899</td>\n",
       "      <td>2.2901</td>\n",
       "      <td>17.0941</td>\n",
       "      <td>-0.1193</td>\n",
       "      <td>13.9055</td>\n",
       "      <td>8.3203</td>\n",
       "      <td>13.0791</td>\n",
       "      <td>-5.1262</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199980</th>\n",
       "      <td>13.4136</td>\n",
       "      <td>5.3912</td>\n",
       "      <td>9.6202</td>\n",
       "      <td>8.5025</td>\n",
       "      <td>12.0951</td>\n",
       "      <td>11.3431</td>\n",
       "      <td>5.8323</td>\n",
       "      <td>12.1429</td>\n",
       "      <td>-3.1511</td>\n",
       "      <td>6.6322</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1563</td>\n",
       "      <td>3.5659</td>\n",
       "      <td>8.0195</td>\n",
       "      <td>16.4657</td>\n",
       "      <td>1.9204</td>\n",
       "      <td>7.3301</td>\n",
       "      <td>9.7953</td>\n",
       "      <td>16.9352</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199981</th>\n",
       "      <td>7.9218</td>\n",
       "      <td>-5.7464</td>\n",
       "      <td>11.4171</td>\n",
       "      <td>6.7972</td>\n",
       "      <td>11.6260</td>\n",
       "      <td>-8.7730</td>\n",
       "      <td>5.4601</td>\n",
       "      <td>12.1401</td>\n",
       "      <td>5.1918</td>\n",
       "      <td>8.2214</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5276</td>\n",
       "      <td>1.8640</td>\n",
       "      <td>3.0752</td>\n",
       "      <td>11.1675</td>\n",
       "      <td>-0.5150</td>\n",
       "      <td>-0.5399</td>\n",
       "      <td>8.1402</td>\n",
       "      <td>19.2653</td>\n",
       "      <td>-30.3989</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199982</th>\n",
       "      <td>7.2189</td>\n",
       "      <td>1.6606</td>\n",
       "      <td>10.4651</td>\n",
       "      <td>4.4382</td>\n",
       "      <td>10.5562</td>\n",
       "      <td>-5.2083</td>\n",
       "      <td>4.7197</td>\n",
       "      <td>10.7883</td>\n",
       "      <td>-8.1002</td>\n",
       "      <td>7.6637</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4636</td>\n",
       "      <td>1.7597</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>15.3558</td>\n",
       "      <td>1.3406</td>\n",
       "      <td>4.2422</td>\n",
       "      <td>8.3144</td>\n",
       "      <td>19.3602</td>\n",
       "      <td>-3.0895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199983</th>\n",
       "      <td>11.8527</td>\n",
       "      <td>5.4321</td>\n",
       "      <td>12.7268</td>\n",
       "      <td>10.2392</td>\n",
       "      <td>12.4740</td>\n",
       "      <td>-14.6939</td>\n",
       "      <td>6.6544</td>\n",
       "      <td>14.1274</td>\n",
       "      <td>-0.4182</td>\n",
       "      <td>8.7811</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4724</td>\n",
       "      <td>1.9564</td>\n",
       "      <td>5.5905</td>\n",
       "      <td>13.4550</td>\n",
       "      <td>-1.7590</td>\n",
       "      <td>3.6424</td>\n",
       "      <td>9.2620</td>\n",
       "      <td>14.0587</td>\n",
       "      <td>5.5770</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199984</th>\n",
       "      <td>12.7445</td>\n",
       "      <td>-6.1135</td>\n",
       "      <td>9.9046</td>\n",
       "      <td>7.5790</td>\n",
       "      <td>14.8852</td>\n",
       "      <td>4.5083</td>\n",
       "      <td>6.3353</td>\n",
       "      <td>21.5936</td>\n",
       "      <td>-4.0102</td>\n",
       "      <td>8.5375</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9345</td>\n",
       "      <td>3.0712</td>\n",
       "      <td>-4.1932</td>\n",
       "      <td>15.1205</td>\n",
       "      <td>-1.0041</td>\n",
       "      <td>-3.9171</td>\n",
       "      <td>9.1933</td>\n",
       "      <td>13.7584</td>\n",
       "      <td>4.3670</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199985</th>\n",
       "      <td>14.8983</td>\n",
       "      <td>2.1302</td>\n",
       "      <td>7.4747</td>\n",
       "      <td>7.1744</td>\n",
       "      <td>11.8252</td>\n",
       "      <td>13.1758</td>\n",
       "      <td>5.1614</td>\n",
       "      <td>13.7914</td>\n",
       "      <td>-4.8184</td>\n",
       "      <td>6.5496</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2747</td>\n",
       "      <td>1.3472</td>\n",
       "      <td>7.6731</td>\n",
       "      <td>19.6489</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>8.7171</td>\n",
       "      <td>8.9479</td>\n",
       "      <td>12.8983</td>\n",
       "      <td>8.3530</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199986</th>\n",
       "      <td>19.2884</td>\n",
       "      <td>-2.8384</td>\n",
       "      <td>11.9149</td>\n",
       "      <td>6.6611</td>\n",
       "      <td>12.3112</td>\n",
       "      <td>12.9244</td>\n",
       "      <td>5.6492</td>\n",
       "      <td>16.0449</td>\n",
       "      <td>5.3597</td>\n",
       "      <td>8.2981</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9924</td>\n",
       "      <td>2.8872</td>\n",
       "      <td>3.3142</td>\n",
       "      <td>22.5225</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>8.3441</td>\n",
       "      <td>14.5823</td>\n",
       "      <td>0.7454</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199987</th>\n",
       "      <td>11.2942</td>\n",
       "      <td>3.6321</td>\n",
       "      <td>15.3300</td>\n",
       "      <td>6.6904</td>\n",
       "      <td>10.9223</td>\n",
       "      <td>-5.6537</td>\n",
       "      <td>6.0221</td>\n",
       "      <td>11.7757</td>\n",
       "      <td>-0.5163</td>\n",
       "      <td>8.9841</td>\n",
       "      <td>...</td>\n",
       "      <td>8.1185</td>\n",
       "      <td>1.3789</td>\n",
       "      <td>9.1349</td>\n",
       "      <td>16.4063</td>\n",
       "      <td>-0.6592</td>\n",
       "      <td>-0.6248</td>\n",
       "      <td>7.9463</td>\n",
       "      <td>14.1967</td>\n",
       "      <td>9.8560</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199988</th>\n",
       "      <td>6.4535</td>\n",
       "      <td>-2.1707</td>\n",
       "      <td>10.7623</td>\n",
       "      <td>8.1571</td>\n",
       "      <td>7.9365</td>\n",
       "      <td>4.6091</td>\n",
       "      <td>4.9564</td>\n",
       "      <td>11.4483</td>\n",
       "      <td>2.8938</td>\n",
       "      <td>6.5602</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3243</td>\n",
       "      <td>1.0464</td>\n",
       "      <td>-0.0645</td>\n",
       "      <td>18.6666</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>-4.3485</td>\n",
       "      <td>9.4572</td>\n",
       "      <td>13.1265</td>\n",
       "      <td>-6.5024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199989</th>\n",
       "      <td>9.0436</td>\n",
       "      <td>-3.0491</td>\n",
       "      <td>10.8737</td>\n",
       "      <td>7.8789</td>\n",
       "      <td>11.0275</td>\n",
       "      <td>-10.1812</td>\n",
       "      <td>6.1978</td>\n",
       "      <td>16.4603</td>\n",
       "      <td>4.4421</td>\n",
       "      <td>9.1971</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6620</td>\n",
       "      <td>1.0286</td>\n",
       "      <td>7.5683</td>\n",
       "      <td>14.6333</td>\n",
       "      <td>-1.2757</td>\n",
       "      <td>-0.3525</td>\n",
       "      <td>8.1526</td>\n",
       "      <td>9.0933</td>\n",
       "      <td>0.8644</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199990</th>\n",
       "      <td>5.5416</td>\n",
       "      <td>1.7340</td>\n",
       "      <td>9.6938</td>\n",
       "      <td>5.0126</td>\n",
       "      <td>11.3049</td>\n",
       "      <td>-15.9906</td>\n",
       "      <td>5.0937</td>\n",
       "      <td>17.7960</td>\n",
       "      <td>-3.1050</td>\n",
       "      <td>6.9197</td>\n",
       "      <td>...</td>\n",
       "      <td>5.4021</td>\n",
       "      <td>1.4266</td>\n",
       "      <td>0.7912</td>\n",
       "      <td>20.6181</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>11.6931</td>\n",
       "      <td>9.6883</td>\n",
       "      <td>12.6723</td>\n",
       "      <td>-16.4310</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>8.7935</td>\n",
       "      <td>-4.0646</td>\n",
       "      <td>9.9480</td>\n",
       "      <td>8.6947</td>\n",
       "      <td>11.0497</td>\n",
       "      <td>-0.5129</td>\n",
       "      <td>5.6410</td>\n",
       "      <td>21.5338</td>\n",
       "      <td>5.6578</td>\n",
       "      <td>5.3441</td>\n",
       "      <td>...</td>\n",
       "      <td>7.4542</td>\n",
       "      <td>4.8159</td>\n",
       "      <td>8.6821</td>\n",
       "      <td>22.1764</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>-1.2475</td>\n",
       "      <td>8.6422</td>\n",
       "      <td>13.7302</td>\n",
       "      <td>-21.5712</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199992</th>\n",
       "      <td>16.4229</td>\n",
       "      <td>-5.0254</td>\n",
       "      <td>13.1385</td>\n",
       "      <td>5.4599</td>\n",
       "      <td>13.1347</td>\n",
       "      <td>-2.6212</td>\n",
       "      <td>4.7829</td>\n",
       "      <td>14.7163</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>8.9048</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3740</td>\n",
       "      <td>0.2961</td>\n",
       "      <td>11.8655</td>\n",
       "      <td>16.2761</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>-6.0102</td>\n",
       "      <td>10.3218</td>\n",
       "      <td>8.2577</td>\n",
       "      <td>5.2651</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199993</th>\n",
       "      <td>14.6764</td>\n",
       "      <td>-8.1066</td>\n",
       "      <td>7.1167</td>\n",
       "      <td>2.4138</td>\n",
       "      <td>10.3845</td>\n",
       "      <td>-11.9327</td>\n",
       "      <td>4.7563</td>\n",
       "      <td>16.0455</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>8.7944</td>\n",
       "      <td>...</td>\n",
       "      <td>7.7472</td>\n",
       "      <td>2.8127</td>\n",
       "      <td>6.6012</td>\n",
       "      <td>15.3706</td>\n",
       "      <td>-0.4293</td>\n",
       "      <td>6.8485</td>\n",
       "      <td>10.4270</td>\n",
       "      <td>17.4970</td>\n",
       "      <td>-13.0074</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>8.2964</td>\n",
       "      <td>-2.3119</td>\n",
       "      <td>11.2139</td>\n",
       "      <td>9.1357</td>\n",
       "      <td>8.5339</td>\n",
       "      <td>4.0350</td>\n",
       "      <td>5.7000</td>\n",
       "      <td>11.0102</td>\n",
       "      <td>4.9089</td>\n",
       "      <td>8.3779</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0753</td>\n",
       "      <td>-0.4822</td>\n",
       "      <td>7.7094</td>\n",
       "      <td>21.5594</td>\n",
       "      <td>-1.2662</td>\n",
       "      <td>4.1468</td>\n",
       "      <td>7.6434</td>\n",
       "      <td>13.0871</td>\n",
       "      <td>-4.3982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>13.1678</td>\n",
       "      <td>1.0136</td>\n",
       "      <td>10.4333</td>\n",
       "      <td>6.7997</td>\n",
       "      <td>8.5974</td>\n",
       "      <td>-4.1641</td>\n",
       "      <td>4.8579</td>\n",
       "      <td>14.7625</td>\n",
       "      <td>-2.7239</td>\n",
       "      <td>6.9937</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6849</td>\n",
       "      <td>4.6734</td>\n",
       "      <td>-1.3660</td>\n",
       "      <td>12.8721</td>\n",
       "      <td>1.2013</td>\n",
       "      <td>-4.6195</td>\n",
       "      <td>9.1568</td>\n",
       "      <td>18.2102</td>\n",
       "      <td>4.8801</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>9.7171</td>\n",
       "      <td>-9.1462</td>\n",
       "      <td>7.3443</td>\n",
       "      <td>9.1421</td>\n",
       "      <td>12.8936</td>\n",
       "      <td>3.0191</td>\n",
       "      <td>5.6888</td>\n",
       "      <td>18.8862</td>\n",
       "      <td>5.0915</td>\n",
       "      <td>6.3545</td>\n",
       "      <td>...</td>\n",
       "      <td>6.6548</td>\n",
       "      <td>1.8197</td>\n",
       "      <td>2.4104</td>\n",
       "      <td>18.9037</td>\n",
       "      <td>-0.9337</td>\n",
       "      <td>2.9995</td>\n",
       "      <td>9.1112</td>\n",
       "      <td>18.1740</td>\n",
       "      <td>-20.7689</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>11.6360</td>\n",
       "      <td>2.2769</td>\n",
       "      <td>11.2074</td>\n",
       "      <td>7.7649</td>\n",
       "      <td>12.6796</td>\n",
       "      <td>11.3224</td>\n",
       "      <td>5.3883</td>\n",
       "      <td>18.3794</td>\n",
       "      <td>1.6603</td>\n",
       "      <td>5.7341</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6498</td>\n",
       "      <td>2.4937</td>\n",
       "      <td>-0.0637</td>\n",
       "      <td>20.0609</td>\n",
       "      <td>-1.1742</td>\n",
       "      <td>-4.1524</td>\n",
       "      <td>9.1933</td>\n",
       "      <td>11.7905</td>\n",
       "      <td>-22.2762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>13.5745</td>\n",
       "      <td>-0.5134</td>\n",
       "      <td>13.6584</td>\n",
       "      <td>7.4855</td>\n",
       "      <td>11.2241</td>\n",
       "      <td>-11.3037</td>\n",
       "      <td>4.1959</td>\n",
       "      <td>16.8280</td>\n",
       "      <td>5.3208</td>\n",
       "      <td>8.9032</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5012</td>\n",
       "      <td>2.2713</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>17.0056</td>\n",
       "      <td>1.1763</td>\n",
       "      <td>-2.3761</td>\n",
       "      <td>8.1079</td>\n",
       "      <td>8.7735</td>\n",
       "      <td>-0.2122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>10.4664</td>\n",
       "      <td>1.8070</td>\n",
       "      <td>10.2277</td>\n",
       "      <td>6.0654</td>\n",
       "      <td>10.0258</td>\n",
       "      <td>1.0789</td>\n",
       "      <td>4.8879</td>\n",
       "      <td>14.4892</td>\n",
       "      <td>-0.5902</td>\n",
       "      <td>7.8362</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2828</td>\n",
       "      <td>1.3601</td>\n",
       "      <td>4.8985</td>\n",
       "      <td>20.0926</td>\n",
       "      <td>-1.3048</td>\n",
       "      <td>-2.5981</td>\n",
       "      <td>10.3378</td>\n",
       "      <td>14.3340</td>\n",
       "      <td>-7.7094</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          var_0    var_1    var_2    var_3    var_4    var_5   var_6    var_7  \\\n",
       "0       11.0656   7.7798  12.9536   9.4292  11.4327  -2.3805  5.8493  18.2675   \n",
       "1        8.5304   1.2543  11.3047   5.1858   9.1974  -4.0117  6.0196  18.6316   \n",
       "2        5.4827 -10.3581  10.1407   7.0479  10.2628   9.8052  4.8950  20.2537   \n",
       "3        8.5374  -1.3222  12.0220   6.5749   8.8458   3.1744  4.9397  20.5660   \n",
       "4       11.7058  -0.1327  14.1295   7.7506   9.1035  -8.5848  6.8595  10.6048   \n",
       "5        5.9862  -2.2913   8.6058   7.0685  14.2465  -8.6761  4.2467  14.7632   \n",
       "6        8.4624  -6.1065   7.3603   8.2627  12.0104  -7.2073  4.1670  13.0809   \n",
       "7       17.3035  -2.4212  13.3989   8.3998  11.0777   9.6449  5.9596  17.8477   \n",
       "8        6.9856   0.8402  13.7161   4.7749   8.6784 -13.7607  4.3386  14.5843   \n",
       "9       10.3811  -6.9348  14.6690   9.0941  11.9058 -10.8018  3.4508  20.2816   \n",
       "10       8.3431  -4.1427   9.1985   9.8229  11.2494   2.9678  5.5184  15.6290   \n",
       "11      10.6137  -2.1898   8.9090   3.8014  13.8602  -5.9802  5.5515  15.4716   \n",
       "12      12.7465  -4.9467  15.5490   6.4580  13.7572 -25.5371  4.4893  15.1682   \n",
       "13      11.7836   1.9979  10.3347   7.8857  13.1020   5.0167  4.9548  23.6527   \n",
       "14       7.0360   1.6797   9.3865   3.2605  10.7569  -8.0802  4.7885  15.0583   \n",
       "15      14.8595  -4.5378  13.6483   5.6480   9.9144   1.5190  5.0358  13.4524   \n",
       "16      14.1732  -5.1490   9.7591   3.7316  10.3700 -21.9202  7.7130  18.8749   \n",
       "17       9.0936  -8.7414  17.1160   6.0126   9.2144  -3.6761  4.6477  20.1053   \n",
       "18      15.7875   0.1671  10.7782   3.8521   9.1190  11.0196  6.1113  18.4368   \n",
       "19      13.3874   1.0716   8.8767   7.8374  11.6404   6.2512  4.8837  18.2178   \n",
       "20       8.0259  -4.6740   8.6431   2.2198  11.4555 -14.0227  6.9192  17.8559   \n",
       "21      14.3356   0.2317   9.5604   5.7603  10.3184  -6.4721  4.6898  13.7783   \n",
       "22      10.4255  -6.1758  12.4846   7.9845   9.7032 -14.5969  4.4173  19.3606   \n",
       "23      12.3322  -6.3835   7.2471   5.0403  10.0875  -1.5252  5.8230  17.9494   \n",
       "24      14.1844  -9.1044   9.7453   9.2638   9.3302   2.6818  5.4711  18.5414   \n",
       "25      10.0029   0.2530   7.5335   6.9343  11.6866  -6.5147  6.7327  19.8941   \n",
       "26       6.9056  -4.8626  11.8932   5.3693  11.2551 -18.9716  5.5991  18.9809   \n",
       "27       8.7562  -3.0647  11.7990   9.2162  10.9847 -22.4902  4.2991  13.9800   \n",
       "28       9.7243  -1.5151  11.5582   5.7360  12.1907   6.9664  4.4125  17.4770   \n",
       "29      13.2430   1.2738  10.4245   3.1863  11.4951  -1.4755  5.1005  17.1687   \n",
       "...         ...      ...      ...      ...      ...      ...     ...      ...   \n",
       "199970  12.7260  -1.6706  12.3598   9.1114  10.1868  -9.5857  5.3494  23.6362   \n",
       "199971   9.4700  -6.7655  12.6591   9.1842  11.8260   0.0264  5.0633  20.7034   \n",
       "199972  13.3243   1.0870   8.4555   3.6929  11.2423   1.3986  4.4765  19.1021   \n",
       "199973  14.2830  -1.8421  11.3664   8.5772   8.8645 -13.8986  4.1603  19.4591   \n",
       "199974   4.5171  -5.2068   7.6007   8.1426  10.4433 -17.2322  4.4205  20.3407   \n",
       "199975  13.4796   2.7000  10.9653   9.1581  13.2959  -3.0995  5.1483  20.9766   \n",
       "199976  12.6337  -6.9793   9.8703   9.9180  10.8092   2.5809  6.7764  18.3443   \n",
       "199977  10.8078  -4.6108   9.0021   9.8910  12.4514  -3.7566  4.2958  19.9677   \n",
       "199978   9.9317  -2.2815  11.1707   5.6826  12.7396  -4.0659  6.2569  12.7697   \n",
       "199979  10.5933  -1.2672  13.6817   6.3789  12.8649  -5.4964  6.4800  13.5986   \n",
       "199980  13.4136   5.3912   9.6202   8.5025  12.0951  11.3431  5.8323  12.1429   \n",
       "199981   7.9218  -5.7464  11.4171   6.7972  11.6260  -8.7730  5.4601  12.1401   \n",
       "199982   7.2189   1.6606  10.4651   4.4382  10.5562  -5.2083  4.7197  10.7883   \n",
       "199983  11.8527   5.4321  12.7268  10.2392  12.4740 -14.6939  6.6544  14.1274   \n",
       "199984  12.7445  -6.1135   9.9046   7.5790  14.8852   4.5083  6.3353  21.5936   \n",
       "199985  14.8983   2.1302   7.4747   7.1744  11.8252  13.1758  5.1614  13.7914   \n",
       "199986  19.2884  -2.8384  11.9149   6.6611  12.3112  12.9244  5.6492  16.0449   \n",
       "199987  11.2942   3.6321  15.3300   6.6904  10.9223  -5.6537  6.0221  11.7757   \n",
       "199988   6.4535  -2.1707  10.7623   8.1571   7.9365   4.6091  4.9564  11.4483   \n",
       "199989   9.0436  -3.0491  10.8737   7.8789  11.0275 -10.1812  6.1978  16.4603   \n",
       "199990   5.5416   1.7340   9.6938   5.0126  11.3049 -15.9906  5.0937  17.7960   \n",
       "199991   8.7935  -4.0646   9.9480   8.6947  11.0497  -0.5129  5.6410  21.5338   \n",
       "199992  16.4229  -5.0254  13.1385   5.4599  13.1347  -2.6212  4.7829  14.7163   \n",
       "199993  14.6764  -8.1066   7.1167   2.4138  10.3845 -11.9327  4.7563  16.0455   \n",
       "199994   8.2964  -2.3119  11.2139   9.1357   8.5339   4.0350  5.7000  11.0102   \n",
       "199995  13.1678   1.0136  10.4333   6.7997   8.5974  -4.1641  4.8579  14.7625   \n",
       "199996   9.7171  -9.1462   7.3443   9.1421  12.8936   3.0191  5.6888  18.8862   \n",
       "199997  11.6360   2.2769  11.2074   7.7649  12.6796  11.3224  5.3883  18.3794   \n",
       "199998  13.5745  -0.5134  13.6584   7.4855  11.2241 -11.3037  4.1959  16.8280   \n",
       "199999  10.4664   1.8070  10.2277   6.0654  10.0258   1.0789  4.8879  14.4892   \n",
       "\n",
       "         var_8   var_9   ...    var_191  var_192  var_193  var_194  var_195  \\\n",
       "0       2.1337  8.8100   ...    11.8495  -1.4300   2.4508  13.7112   2.4669   \n",
       "1      -4.4131  5.9739   ...     8.8349   0.9403  10.1282  15.5765   0.4773   \n",
       "2       1.5233  8.3442   ...    10.9935   1.9803   2.1800  12.9813   2.1281   \n",
       "3       3.3755  7.4578   ...     9.0766   1.6580   3.5813  15.1874   3.1656   \n",
       "4       2.9890  7.1437   ...     9.1723   1.2835   3.3778  19.5542  -0.2860   \n",
       "5       1.8790  7.2842   ...     7.1178  -0.4249   8.8781  14.9438  -2.2151   \n",
       "6      -4.3004  6.3181   ...     6.8661   4.0971   8.8484  17.5010   0.0295   \n",
       "7      -4.8068  7.4643   ...     4.4214   0.9303   1.4994  15.2648  -1.7931   \n",
       "8       2.5883  7.2215   ...     7.8754   2.4698  -0.0362  16.7144   0.1221   \n",
       "9      -1.4112  6.7401   ...    11.0723   0.8907   4.7680  15.1425   0.6075   \n",
       "10      2.8032  8.9513   ...     6.7916   0.5810  -2.1353  23.2482   0.8738   \n",
       "11     -0.1714  7.6178   ...     4.0625  -0.1537   7.9787  18.4518   0.1000   \n",
       "12      3.1754  7.5722   ...     4.3879   1.9888  -0.6670  17.6003  -0.7705   \n",
       "13      3.5911  5.8546   ...     7.3561   3.2395   3.9152  16.6549   0.4106   \n",
       "14      0.6459  4.8661   ...     3.0835   2.7034   0.9803  18.8216  -0.8522   \n",
       "15     -2.5419  9.4450   ...     5.8526   4.8517   2.5020  22.8224  -0.9325   \n",
       "16      0.4680  7.8453   ...     5.9058   1.3140   4.8961  20.1087   1.1051   \n",
       "17      1.7687  7.9974   ...     7.7219   0.4559   2.7977  21.4898  -0.2385   \n",
       "18     -1.0728  7.0586   ...    13.1753   0.2111   6.6279  19.2890  -0.0982   \n",
       "19      4.3871  9.5648   ...    11.8869   1.6137   4.7306  15.0826  -0.2445   \n",
       "20      0.4283  6.5548   ...     3.9088  -0.2736   5.4434  19.9933  -0.6241   \n",
       "21      1.8342  9.2284   ...     6.8724   1.5538   4.6148  13.1176   0.0117   \n",
       "22     -0.5899  6.9213   ...     4.7884   5.0819  -5.1081  16.3797  -0.1113   \n",
       "23     -3.8454  6.2356   ...    11.1389   1.4025  -0.4134  16.9806   0.1357   \n",
       "24      2.2065  8.3338   ...     9.4174   0.7743   4.5556  17.7824  -0.2536   \n",
       "25     -6.6497  8.0114   ...     5.1404   1.2851   2.2366  23.5212   1.3798   \n",
       "26      5.5612  7.8337   ...     6.7125   0.3169   6.0765  17.6562   2.1001   \n",
       "27      3.3233  7.7593   ...     4.7817   1.3885  -1.3448  23.1983  -1.5301   \n",
       "28     -3.9683  7.5912   ...    11.2645   3.1891  -0.0828  18.2102   1.2457   \n",
       "29      1.7115  9.1463   ...     7.8381   0.9822   2.2038  20.3427   1.1092   \n",
       "...        ...     ...   ...        ...      ...      ...      ...      ...   \n",
       "199970  2.0626  6.2033   ...     9.5029  -0.1536   1.9221  11.4131   0.6650   \n",
       "199971  2.4171  8.2646   ...    11.5775   1.4309   8.0710  15.0869  -0.9276   \n",
       "199972 -2.6573  8.6612   ...     9.2868   3.1847   8.8422  15.4298  -1.3178   \n",
       "199973  5.6445  9.2011   ...     9.2002   3.3686   6.2250  21.0475  -2.0668   \n",
       "199974 -1.0196  5.6569   ...     3.4422   3.0645   1.2469  14.8673  -0.4562   \n",
       "199975  1.2932  7.6743   ...     4.1240   2.5431   6.2910  14.6021  -3.0392   \n",
       "199976  4.1498  7.8825   ...    11.0722   1.2368   0.6363  17.7793   1.1790   \n",
       "199977  0.8806  8.2828   ...     3.9763  -0.6260  -0.8544  21.5958   0.3250   \n",
       "199978 -2.1645  8.9019   ...     3.5345   1.4136   7.5265  20.9985   1.7101   \n",
       "199979  4.0315  8.8308   ...     6.5479   1.1899   2.2901  17.0941  -0.1193   \n",
       "199980 -3.1511  6.6322   ...     3.1563   3.5659   8.0195  16.4657   1.9204   \n",
       "199981  5.1918  8.2214   ...     8.5276   1.8640   3.0752  11.1675  -0.5150   \n",
       "199982 -8.1002  7.6637   ...     3.4636   1.7597   0.9145  15.3558   1.3406   \n",
       "199983 -0.4182  8.7811   ...     4.4724   1.9564   5.5905  13.4550  -1.7590   \n",
       "199984 -4.0102  8.5375   ...     9.9345   3.0712  -4.1932  15.1205  -1.0041   \n",
       "199985 -4.8184  6.5496   ...     4.2747   1.3472   7.6731  19.6489   0.0318   \n",
       "199986  5.3597  8.2981   ...     3.9924   2.8872   3.3142  22.5225   0.9812   \n",
       "199987 -0.5163  8.9841   ...     8.1185   1.3789   9.1349  16.4063  -0.6592   \n",
       "199988  2.8938  6.5602   ...     8.3243   1.0464  -0.0645  18.6666   0.6250   \n",
       "199989  4.4421  9.1971   ...     5.6620   1.0286   7.5683  14.6333  -1.2757   \n",
       "199990 -3.1050  6.9197   ...     5.4021   1.4266   0.7912  20.6181   0.5917   \n",
       "199991  5.6578  5.3441   ...     7.4542   4.8159   8.6821  22.1764   0.0088   \n",
       "199992  0.0779  8.9048   ...     4.3740   0.2961  11.8655  16.2761   0.0447   \n",
       "199993  0.4510  8.7944   ...     7.7472   2.8127   6.6012  15.3706  -0.4293   \n",
       "199994  4.9089  8.3779   ...    10.0753  -0.4822   7.7094  21.5594  -1.2662   \n",
       "199995 -2.7239  6.9937   ...     9.6849   4.6734  -1.3660  12.8721   1.2013   \n",
       "199996  5.0915  6.3545   ...     6.6548   1.8197   2.4104  18.9037  -0.9337   \n",
       "199997  1.6603  5.7341   ...     2.6498   2.4937  -0.0637  20.0609  -1.1742   \n",
       "199998  5.3208  8.9032   ...     8.5012   2.2713   5.7621  17.0056   1.1763   \n",
       "199999 -0.5902  7.8362   ...     9.2828   1.3601   4.8985  20.0926  -1.3048   \n",
       "\n",
       "        var_196  var_197  var_198  var_199  kmeans  \n",
       "0        4.3654  10.7200  15.4722  -8.7197       2  \n",
       "1       -1.4852   9.8714  19.1293 -20.9760       8  \n",
       "2       -7.1086   7.0618  19.8956 -23.1794       3  \n",
       "3        3.9567   9.2295  13.0168  -4.2108       6  \n",
       "4       -5.1612   7.2882  13.9260  -9.1846       5  \n",
       "5       -6.0233   9.8117  17.1127  10.8240       7  \n",
       "6        7.7443   9.1509  18.4736   5.1499       5  \n",
       "7        6.5316  10.4855  23.4631   0.7283       4  \n",
       "8       -1.4328   9.9207  16.9865  -3.3304       7  \n",
       "9       -4.4447   9.5788  15.8146   9.3457       6  \n",
       "10       4.6857  10.1361  12.1140  -2.4978       1  \n",
       "11      -7.8212   9.2355  15.0721  -7.3475       4  \n",
       "12       6.5424   8.9599  16.9317 -14.0779       1  \n",
       "13      -2.1647  10.0379  20.5904   6.0166       0  \n",
       "14      10.3456   8.2077  17.6097  -0.9141       7  \n",
       "15       8.6849  10.2848  17.4932   6.0800       5  \n",
       "16       7.7184   9.3406  21.1746  -2.0098       7  \n",
       "17       8.1878   9.3114  23.0545  -2.3171       0  \n",
       "18       7.5500   9.3848  14.4007  11.2567       3  \n",
       "19       2.1377   8.1116  20.5129   5.4945       8  \n",
       "20      11.7708   7.8690  18.1980  -3.7655       0  \n",
       "21       9.3434  10.7644  18.0992  -7.6905       1  \n",
       "22       4.3362   9.0132  16.9635   3.2523       4  \n",
       "23      13.9872   8.2489  22.4405  -5.4524       3  \n",
       "24       7.3867   7.9695  12.6078   7.3475       1  \n",
       "25       7.6392   6.9239  18.6460 -17.7609       4  \n",
       "26      -1.5770   8.0712  16.6316   7.8962       8  \n",
       "27       3.9577   8.7277  16.8255 -18.6596       1  \n",
       "28       1.9391   8.6899  14.5739  16.2884       7  \n",
       "29      15.0418   8.4667  15.1478   6.2962       8  \n",
       "...         ...      ...      ...      ...     ...  \n",
       "199970  -5.2401   9.1059  19.3888  -7.6292       0  \n",
       "199971  -0.5783   8.2742  18.2273  -1.0124       1  \n",
       "199972  -9.4033   9.7116  15.5697  -0.7362       6  \n",
       "199973   0.7093   7.8924  16.0279   7.8535       9  \n",
       "199974  13.9675   8.0088  14.3127  -1.2940       8  \n",
       "199975   4.8943   8.7922  19.6428 -24.7859       2  \n",
       "199976  -2.0814  10.6610  11.5385 -23.6067       8  \n",
       "199977   7.5404  10.0748  16.0592 -14.0012       3  \n",
       "199978   7.9362   7.7696  10.9202 -23.5055       2  \n",
       "199979  13.9055   8.3203  13.0791  -5.1262       3  \n",
       "199980   7.3301   9.7953  16.9352   0.2655       8  \n",
       "199981  -0.5399   8.1402  19.2653 -30.3989       6  \n",
       "199982   4.2422   8.3144  19.3602  -3.0895       0  \n",
       "199983   3.6424   9.2620  14.0587   5.5770       6  \n",
       "199984  -3.9171   9.1933  13.7584   4.3670       3  \n",
       "199985   8.7171   8.9479  12.8983   8.3530       2  \n",
       "199986   0.1020   8.3441  14.5823   0.7454       4  \n",
       "199987  -0.6248   7.9463  14.1967   9.8560       9  \n",
       "199988  -4.3485   9.4572  13.1265  -6.5024       4  \n",
       "199989  -0.3525   8.1526   9.0933   0.8644       4  \n",
       "199990  11.6931   9.6883  12.6723 -16.4310       4  \n",
       "199991  -1.2475   8.6422  13.7302 -21.5712       9  \n",
       "199992  -6.0102  10.3218   8.2577   5.2651       6  \n",
       "199993   6.8485  10.4270  17.4970 -13.0074       6  \n",
       "199994   4.1468   7.6434  13.0871  -4.3982       1  \n",
       "199995  -4.6195   9.1568  18.2102   4.8801       0  \n",
       "199996   2.9995   9.1112  18.1740 -20.7689       2  \n",
       "199997  -4.1524   9.1933  11.7905 -22.2762       0  \n",
       "199998  -2.3761   8.1079   8.7735  -0.2122       1  \n",
       "199999  -2.5981  10.3378  14.3340  -7.7094       7  \n",
       "\n",
       "[200000 rows x 201 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_param\n",
    "param = {\n",
    "    \"objective\" : \"binary\", \n",
    "    \"boost\":\"gbdt\",\n",
    "    \"metric\":\"auc\",\n",
    "    \"boost_from_average\":\"false\",\n",
    "    \"num_threads\":28,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"num_leaves\" : 13,\n",
    "    \"max_depth\":-1,\n",
    "    \"tree_learner\" : \"serial\",\n",
    "    \"feature_fraction\" : 0.05,\n",
    "    \"bagging_freq\" : 5,\n",
    "    \"bagging_fraction\" : 0.4,\n",
    "    \"min_data_in_leaf\" : 80,\n",
    "    \"min_sum_hessian_in_leaf\" : 10.0,\n",
    "    \"verbosity\" : 1,\n",
    "    'seed': 44000,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm(train_x,train_y,test_x,test_y):\n",
    "    trn_data = lgb.Dataset(train_x,train_y)\n",
    "    val_data = lgb.Dataset(test_x,test_y)\n",
    "    \n",
    "    num_round = 1000000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n",
    "    \n",
    "    val_pred = clf.predict(test_x, num_iteration=clf.best_iteration)\n",
    "    auc_score=roc_auc_score(test_y, val_pred)\n",
    "    print(\"\")\n",
    "    print(\"AUC = {}\".format(auc_score))\n",
    "    \n",
    "    return auc_score,clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_clf(pred_x,pred_y,other_x,other_y):\n",
    "    print(\"\")\n",
    "    print('pred_num='+str(len(pred_x)))\n",
    "    print('other_num='+str(len(other_x)))\n",
    "    max_score=0\n",
    "    best_i=0\n",
    "    print(\"\")\n",
    "    for i in range(0,3):\n",
    "        #model_param\n",
    "        param = {\n",
    "            \"objective\" : \"binary\", \n",
    "            \"boost\":\"gbdt\",\n",
    "            \"metric\":\"auc\",\n",
    "            \"boost_from_average\":\"false\",\n",
    "            \"num_threads\":28,\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"num_leaves\" : 13,\n",
    "            \"max_depth\":-1,\n",
    "            \"tree_learner\" : \"serial\",\n",
    "            \"feature_fraction\" : 0.05,\n",
    "            \"bagging_freq\" : 5,\n",
    "            \"bagging_fraction\" : 0.4,\n",
    "            \"min_data_in_leaf\" : 80,\n",
    "            \"min_sum_hessian_in_leaf\" : 10.0,\n",
    "            \"verbosity\" : 1,\n",
    "            'seed': i,\n",
    "            }\n",
    "        sample_num=int(len(other_x)*0.9)\n",
    "        print(\"\")\n",
    "        print('sample_num='+str(sample_num))\n",
    "        other_sample_x=other_x.sample(n=sample_num,random_state=i)\n",
    "        other_sample_y=other_y.sample(n=sample_num,random_state=i)\n",
    "        score,model=lgbm(other_sample_x,other_sample_y,pred_x,pred_y)\n",
    "        if max_score < score :\n",
    "            max_score=score\n",
    "            pred_model=model\n",
    "            best_i=i\n",
    "    print(\"\")\n",
    "    print('Best Model:'+str(i)+\" Best Score:\"+str(max_score))\n",
    "    return pred_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['target']=y_train\n",
    "pred_x=x_train.query('kmeans == 0')\n",
    "other_x=x_train.query('not kmeans == 0')\n",
    "pred_y=pred_x['target']\n",
    "other_y=other_x['target']\n",
    "pred_x=pred_x.drop('target',axis=1)\n",
    "other_x=other_x.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kmeans:0\n",
      "\n",
      "pred_num=20763\n",
      "other_num=179237\n",
      "\n",
      "\n",
      "sample_num=161313\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901769\tvalid_1's auc: 0.881983\n",
      "[2000]\ttraining's auc: 0.913408\tvalid_1's auc: 0.89058\n",
      "[3000]\ttraining's auc: 0.921125\tvalid_1's auc: 0.895256\n",
      "[4000]\ttraining's auc: 0.92718\tvalid_1's auc: 0.898192\n",
      "[5000]\ttraining's auc: 0.932299\tvalid_1's auc: 0.899885\n",
      "[6000]\ttraining's auc: 0.936735\tvalid_1's auc: 0.900649\n",
      "[7000]\ttraining's auc: 0.941023\tvalid_1's auc: 0.900961\n",
      "[8000]\ttraining's auc: 0.944909\tvalid_1's auc: 0.901262\n",
      "[9000]\ttraining's auc: 0.948601\tvalid_1's auc: 0.901544\n",
      "[10000]\ttraining's auc: 0.952124\tvalid_1's auc: 0.901436\n",
      "[11000]\ttraining's auc: 0.955533\tvalid_1's auc: 0.901393\n",
      "[12000]\ttraining's auc: 0.958782\tvalid_1's auc: 0.901492\n",
      "Early stopping, best iteration is:\n",
      "[9099]\ttraining's auc: 0.948953\tvalid_1's auc: 0.901594\n",
      "\n",
      "AUC = 0.9015939773459362\n",
      "\n",
      "sample_num=161313\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.902821\tvalid_1's auc: 0.883077\n",
      "[2000]\ttraining's auc: 0.914409\tvalid_1's auc: 0.891329\n",
      "[3000]\ttraining's auc: 0.922167\tvalid_1's auc: 0.896088\n",
      "[4000]\ttraining's auc: 0.928048\tvalid_1's auc: 0.898374\n",
      "[5000]\ttraining's auc: 0.933186\tvalid_1's auc: 0.899853\n",
      "[6000]\ttraining's auc: 0.937637\tvalid_1's auc: 0.900712\n",
      "[7000]\ttraining's auc: 0.941801\tvalid_1's auc: 0.90108\n",
      "[8000]\ttraining's auc: 0.945643\tvalid_1's auc: 0.901223\n",
      "[9000]\ttraining's auc: 0.94933\tvalid_1's auc: 0.90137\n",
      "[10000]\ttraining's auc: 0.952878\tvalid_1's auc: 0.90127\n",
      "[11000]\ttraining's auc: 0.956226\tvalid_1's auc: 0.901271\n",
      "[12000]\ttraining's auc: 0.959409\tvalid_1's auc: 0.90111\n",
      "Early stopping, best iteration is:\n",
      "[9056]\ttraining's auc: 0.949542\tvalid_1's auc: 0.901403\n",
      "\n",
      "AUC = 0.9014025709296603\n",
      "\n",
      "sample_num=161313\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901074\tvalid_1's auc: 0.881812\n",
      "[2000]\ttraining's auc: 0.913264\tvalid_1's auc: 0.890962\n",
      "[3000]\ttraining's auc: 0.921082\tvalid_1's auc: 0.895397\n",
      "[4000]\ttraining's auc: 0.927153\tvalid_1's auc: 0.898104\n",
      "[5000]\ttraining's auc: 0.932324\tvalid_1's auc: 0.899558\n",
      "[6000]\ttraining's auc: 0.936903\tvalid_1's auc: 0.900545\n",
      "[7000]\ttraining's auc: 0.941147\tvalid_1's auc: 0.900821\n",
      "[8000]\ttraining's auc: 0.945116\tvalid_1's auc: 0.90126\n",
      "[9000]\ttraining's auc: 0.948834\tvalid_1's auc: 0.901618\n",
      "[10000]\ttraining's auc: 0.952408\tvalid_1's auc: 0.901699\n",
      "[11000]\ttraining's auc: 0.955798\tvalid_1's auc: 0.9015\n",
      "[12000]\ttraining's auc: 0.959011\tvalid_1's auc: 0.901243\n",
      "Early stopping, best iteration is:\n",
      "[9820]\ttraining's auc: 0.951803\tvalid_1's auc: 0.901732\n",
      "\n",
      "AUC = 0.901732127629148\n",
      "\n",
      "Best Model:2 Best Score:0.901732127629148\n",
      "\n",
      "kmeans:1\n",
      "\n",
      "pred_num=19068\n",
      "other_num=180932\n",
      "\n",
      "\n",
      "sample_num=162838\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901992\tvalid_1's auc: 0.880567\n",
      "[2000]\ttraining's auc: 0.91404\tvalid_1's auc: 0.888214\n",
      "[3000]\ttraining's auc: 0.922078\tvalid_1's auc: 0.892103\n",
      "[4000]\ttraining's auc: 0.92809\tvalid_1's auc: 0.894652\n",
      "[5000]\ttraining's auc: 0.933115\tvalid_1's auc: 0.895829\n",
      "[6000]\ttraining's auc: 0.937623\tvalid_1's auc: 0.896583\n",
      "[7000]\ttraining's auc: 0.941697\tvalid_1's auc: 0.89689\n",
      "[8000]\ttraining's auc: 0.945576\tvalid_1's auc: 0.896995\n",
      "[9000]\ttraining's auc: 0.949268\tvalid_1's auc: 0.897078\n",
      "[10000]\ttraining's auc: 0.952755\tvalid_1's auc: 0.897056\n",
      "Early stopping, best iteration is:\n",
      "[7726]\ttraining's auc: 0.944514\tvalid_1's auc: 0.897136\n",
      "\n",
      "AUC = 0.8971359162891761\n",
      "\n",
      "sample_num=162838\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900909\tvalid_1's auc: 0.880912\n",
      "[2000]\ttraining's auc: 0.912737\tvalid_1's auc: 0.888188\n",
      "[3000]\ttraining's auc: 0.920776\tvalid_1's auc: 0.892239\n",
      "[4000]\ttraining's auc: 0.926811\tvalid_1's auc: 0.894847\n",
      "[5000]\ttraining's auc: 0.93201\tvalid_1's auc: 0.896029\n",
      "[6000]\ttraining's auc: 0.936496\tvalid_1's auc: 0.896537\n",
      "[7000]\ttraining's auc: 0.940686\tvalid_1's auc: 0.896808\n",
      "[8000]\ttraining's auc: 0.944621\tvalid_1's auc: 0.89699\n",
      "[9000]\ttraining's auc: 0.948424\tvalid_1's auc: 0.896986\n",
      "[10000]\ttraining's auc: 0.951996\tvalid_1's auc: 0.89703\n",
      "[11000]\ttraining's auc: 0.955319\tvalid_1's auc: 0.897097\n",
      "Early stopping, best iteration is:\n",
      "[8429]\ttraining's auc: 0.946282\tvalid_1's auc: 0.897191\n",
      "\n",
      "AUC = 0.8971913087476278\n",
      "\n",
      "sample_num=162838\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901542\tvalid_1's auc: 0.882308\n",
      "[2000]\ttraining's auc: 0.913556\tvalid_1's auc: 0.889395\n",
      "[3000]\ttraining's auc: 0.92144\tvalid_1's auc: 0.893252\n",
      "[4000]\ttraining's auc: 0.927442\tvalid_1's auc: 0.895441\n",
      "[5000]\ttraining's auc: 0.932679\tvalid_1's auc: 0.89654\n",
      "[6000]\ttraining's auc: 0.937151\tvalid_1's auc: 0.897087\n",
      "[7000]\ttraining's auc: 0.941338\tvalid_1's auc: 0.89759\n",
      "[8000]\ttraining's auc: 0.945246\tvalid_1's auc: 0.897664\n",
      "[9000]\ttraining's auc: 0.948962\tvalid_1's auc: 0.897823\n",
      "[10000]\ttraining's auc: 0.95248\tvalid_1's auc: 0.89775\n",
      "[11000]\ttraining's auc: 0.955932\tvalid_1's auc: 0.897839\n",
      "[12000]\ttraining's auc: 0.959162\tvalid_1's auc: 0.897841\n",
      "Early stopping, best iteration is:\n",
      "[9452]\ttraining's auc: 0.950562\tvalid_1's auc: 0.897904\n",
      "\n",
      "AUC = 0.8979043850667698\n",
      "\n",
      "Best Model:2 Best Score:0.8979043850667698\n",
      "\n",
      "kmeans:2\n",
      "\n",
      "pred_num=20202\n",
      "other_num=179798\n",
      "\n",
      "\n",
      "sample_num=161818\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900817\tvalid_1's auc: 0.888218\n",
      "[2000]\ttraining's auc: 0.912898\tvalid_1's auc: 0.895637\n",
      "[3000]\ttraining's auc: 0.920954\tvalid_1's auc: 0.898966\n",
      "[4000]\ttraining's auc: 0.92697\tvalid_1's auc: 0.900754\n",
      "[5000]\ttraining's auc: 0.931989\tvalid_1's auc: 0.901725\n",
      "[6000]\ttraining's auc: 0.936498\tvalid_1's auc: 0.902114\n",
      "[7000]\ttraining's auc: 0.940657\tvalid_1's auc: 0.902415\n",
      "[8000]\ttraining's auc: 0.944572\tvalid_1's auc: 0.9024\n",
      "[9000]\ttraining's auc: 0.948206\tvalid_1's auc: 0.902441\n",
      "[10000]\ttraining's auc: 0.951794\tvalid_1's auc: 0.902313\n",
      "[11000]\ttraining's auc: 0.955192\tvalid_1's auc: 0.902059\n",
      "Early stopping, best iteration is:\n",
      "[8595]\ttraining's auc: 0.946741\tvalid_1's auc: 0.902533\n",
      "\n",
      "AUC = 0.9025327853068987\n",
      "\n",
      "sample_num=161818\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.9014\tvalid_1's auc: 0.888098\n",
      "[2000]\ttraining's auc: 0.913392\tvalid_1's auc: 0.895827\n",
      "[3000]\ttraining's auc: 0.921203\tvalid_1's auc: 0.899149\n",
      "[4000]\ttraining's auc: 0.92713\tvalid_1's auc: 0.900663\n",
      "[5000]\ttraining's auc: 0.932261\tvalid_1's auc: 0.901795\n",
      "[6000]\ttraining's auc: 0.936686\tvalid_1's auc: 0.902168\n",
      "[7000]\ttraining's auc: 0.94088\tvalid_1's auc: 0.902364\n",
      "[8000]\ttraining's auc: 0.944835\tvalid_1's auc: 0.9023\n",
      "[9000]\ttraining's auc: 0.948544\tvalid_1's auc: 0.902234\n",
      "Early stopping, best iteration is:\n",
      "[6988]\ttraining's auc: 0.940831\tvalid_1's auc: 0.902367\n",
      "\n",
      "AUC = 0.9023668699322099\n",
      "\n",
      "sample_num=161818\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900829\tvalid_1's auc: 0.888737\n",
      "[2000]\ttraining's auc: 0.91265\tvalid_1's auc: 0.896222\n",
      "[3000]\ttraining's auc: 0.920655\tvalid_1's auc: 0.899831\n",
      "[4000]\ttraining's auc: 0.926744\tvalid_1's auc: 0.901687\n",
      "[5000]\ttraining's auc: 0.93196\tvalid_1's auc: 0.902866\n",
      "[6000]\ttraining's auc: 0.936504\tvalid_1's auc: 0.903307\n",
      "[7000]\ttraining's auc: 0.940749\tvalid_1's auc: 0.903626\n",
      "[8000]\ttraining's auc: 0.944713\tvalid_1's auc: 0.90384\n",
      "[9000]\ttraining's auc: 0.948437\tvalid_1's auc: 0.903616\n",
      "[10000]\ttraining's auc: 0.952006\tvalid_1's auc: 0.903497\n",
      "[11000]\ttraining's auc: 0.955361\tvalid_1's auc: 0.903328\n",
      "Early stopping, best iteration is:\n",
      "[8124]\ttraining's auc: 0.945183\tvalid_1's auc: 0.903852\n",
      "\n",
      "AUC = 0.9038520135639972\n",
      "\n",
      "Best Model:2 Best Score:0.9038520135639972\n",
      "\n",
      "kmeans:3\n",
      "\n",
      "pred_num=19034\n",
      "other_num=180966\n",
      "\n",
      "\n",
      "sample_num=162869\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900822\tvalid_1's auc: 0.882621\n",
      "[2000]\ttraining's auc: 0.912745\tvalid_1's auc: 0.890821\n",
      "[3000]\ttraining's auc: 0.92053\tvalid_1's auc: 0.894595\n",
      "[4000]\ttraining's auc: 0.926562\tvalid_1's auc: 0.896732\n",
      "[5000]\ttraining's auc: 0.931761\tvalid_1's auc: 0.89795\n",
      "[6000]\ttraining's auc: 0.936241\tvalid_1's auc: 0.89897\n",
      "[7000]\ttraining's auc: 0.940451\tvalid_1's auc: 0.899355\n",
      "[8000]\ttraining's auc: 0.94429\tvalid_1's auc: 0.899534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9000]\ttraining's auc: 0.948054\tvalid_1's auc: 0.89957\n",
      "[10000]\ttraining's auc: 0.951536\tvalid_1's auc: 0.899633\n",
      "[11000]\ttraining's auc: 0.954889\tvalid_1's auc: 0.8996\n",
      "[12000]\ttraining's auc: 0.958105\tvalid_1's auc: 0.899555\n",
      "[13000]\ttraining's auc: 0.961164\tvalid_1's auc: 0.899514\n",
      "[14000]\ttraining's auc: 0.964096\tvalid_1's auc: 0.899207\n",
      "Early stopping, best iteration is:\n",
      "[11185]\ttraining's auc: 0.955493\tvalid_1's auc: 0.899702\n",
      "\n",
      "AUC = 0.8997015523442671\n",
      "\n",
      "sample_num=162869\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.9013\tvalid_1's auc: 0.883276\n",
      "[2000]\ttraining's auc: 0.913015\tvalid_1's auc: 0.890322\n",
      "[3000]\ttraining's auc: 0.920884\tvalid_1's auc: 0.89404\n",
      "[4000]\ttraining's auc: 0.926874\tvalid_1's auc: 0.89631\n",
      "[5000]\ttraining's auc: 0.932027\tvalid_1's auc: 0.897613\n",
      "[6000]\ttraining's auc: 0.936572\tvalid_1's auc: 0.898706\n",
      "[7000]\ttraining's auc: 0.940799\tvalid_1's auc: 0.899087\n",
      "[8000]\ttraining's auc: 0.944698\tvalid_1's auc: 0.899325\n",
      "[9000]\ttraining's auc: 0.948343\tvalid_1's auc: 0.899253\n",
      "[10000]\ttraining's auc: 0.951821\tvalid_1's auc: 0.899311\n",
      "[11000]\ttraining's auc: 0.955255\tvalid_1's auc: 0.89919\n",
      "[12000]\ttraining's auc: 0.958512\tvalid_1's auc: 0.898855\n",
      "[13000]\ttraining's auc: 0.961533\tvalid_1's auc: 0.898698\n",
      "Early stopping, best iteration is:\n",
      "[10155]\ttraining's auc: 0.952356\tvalid_1's auc: 0.899394\n",
      "\n",
      "AUC = 0.8993942149287631\n",
      "\n",
      "sample_num=162869\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900956\tvalid_1's auc: 0.884197\n",
      "[2000]\ttraining's auc: 0.912706\tvalid_1's auc: 0.891292\n",
      "[3000]\ttraining's auc: 0.920538\tvalid_1's auc: 0.894951\n",
      "[4000]\ttraining's auc: 0.92657\tvalid_1's auc: 0.897051\n",
      "[5000]\ttraining's auc: 0.931735\tvalid_1's auc: 0.898309\n",
      "[6000]\ttraining's auc: 0.936218\tvalid_1's auc: 0.899162\n",
      "[7000]\ttraining's auc: 0.940386\tvalid_1's auc: 0.899447\n",
      "[8000]\ttraining's auc: 0.944232\tvalid_1's auc: 0.899665\n",
      "[9000]\ttraining's auc: 0.947898\tvalid_1's auc: 0.899757\n",
      "[10000]\ttraining's auc: 0.951437\tvalid_1's auc: 0.899651\n",
      "[11000]\ttraining's auc: 0.954789\tvalid_1's auc: 0.89968\n",
      "[12000]\ttraining's auc: 0.958001\tvalid_1's auc: 0.899553\n",
      "Early stopping, best iteration is:\n",
      "[9114]\ttraining's auc: 0.948323\tvalid_1's auc: 0.899839\n",
      "\n",
      "AUC = 0.8998393703166917\n",
      "\n",
      "Best Model:2 Best Score:0.8998393703166917\n",
      "\n",
      "kmeans:4\n",
      "\n",
      "pred_num=20902\n",
      "other_num=179098\n",
      "\n",
      "\n",
      "sample_num=161188\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.902251\tvalid_1's auc: 0.878074\n",
      "[2000]\ttraining's auc: 0.913913\tvalid_1's auc: 0.886735\n",
      "[3000]\ttraining's auc: 0.921706\tvalid_1's auc: 0.891186\n",
      "[4000]\ttraining's auc: 0.927576\tvalid_1's auc: 0.893663\n",
      "[5000]\ttraining's auc: 0.93282\tvalid_1's auc: 0.895197\n",
      "[6000]\ttraining's auc: 0.937313\tvalid_1's auc: 0.896159\n",
      "[7000]\ttraining's auc: 0.941537\tvalid_1's auc: 0.896856\n",
      "[8000]\ttraining's auc: 0.945349\tvalid_1's auc: 0.897133\n",
      "[9000]\ttraining's auc: 0.949135\tvalid_1's auc: 0.897083\n",
      "[10000]\ttraining's auc: 0.952676\tvalid_1's auc: 0.897224\n",
      "[11000]\ttraining's auc: 0.956117\tvalid_1's auc: 0.89723\n",
      "[12000]\ttraining's auc: 0.959361\tvalid_1's auc: 0.897174\n",
      "[13000]\ttraining's auc: 0.962404\tvalid_1's auc: 0.897003\n",
      "Early stopping, best iteration is:\n",
      "[10468]\ttraining's auc: 0.954294\tvalid_1's auc: 0.897282\n",
      "\n",
      "AUC = 0.8972817112442295\n",
      "\n",
      "sample_num=161188\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901936\tvalid_1's auc: 0.877318\n",
      "[2000]\ttraining's auc: 0.913758\tvalid_1's auc: 0.885381\n",
      "[3000]\ttraining's auc: 0.921505\tvalid_1's auc: 0.890013\n",
      "[4000]\ttraining's auc: 0.92753\tvalid_1's auc: 0.892826\n",
      "[5000]\ttraining's auc: 0.932749\tvalid_1's auc: 0.894497\n",
      "[6000]\ttraining's auc: 0.937288\tvalid_1's auc: 0.895461\n",
      "[7000]\ttraining's auc: 0.94146\tvalid_1's auc: 0.896135\n",
      "[8000]\ttraining's auc: 0.945345\tvalid_1's auc: 0.896316\n",
      "[9000]\ttraining's auc: 0.949007\tvalid_1's auc: 0.896478\n",
      "[10000]\ttraining's auc: 0.952523\tvalid_1's auc: 0.896395\n",
      "[11000]\ttraining's auc: 0.955979\tvalid_1's auc: 0.896619\n",
      "[12000]\ttraining's auc: 0.959225\tvalid_1's auc: 0.896581\n",
      "[13000]\ttraining's auc: 0.962332\tvalid_1's auc: 0.89645\n",
      "[14000]\ttraining's auc: 0.965225\tvalid_1's auc: 0.896358\n",
      "Early stopping, best iteration is:\n",
      "[11566]\ttraining's auc: 0.957847\tvalid_1's auc: 0.896699\n",
      "\n",
      "AUC = 0.8966991459066787\n",
      "\n",
      "sample_num=161188\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901865\tvalid_1's auc: 0.878557\n",
      "[2000]\ttraining's auc: 0.913781\tvalid_1's auc: 0.886374\n",
      "[3000]\ttraining's auc: 0.921439\tvalid_1's auc: 0.890785\n",
      "[4000]\ttraining's auc: 0.92749\tvalid_1's auc: 0.893526\n",
      "[5000]\ttraining's auc: 0.932507\tvalid_1's auc: 0.89511\n",
      "[6000]\ttraining's auc: 0.937073\tvalid_1's auc: 0.895831\n",
      "[7000]\ttraining's auc: 0.941245\tvalid_1's auc: 0.896349\n",
      "[8000]\ttraining's auc: 0.945104\tvalid_1's auc: 0.896541\n",
      "[9000]\ttraining's auc: 0.948801\tvalid_1's auc: 0.896691\n",
      "[10000]\ttraining's auc: 0.952323\tvalid_1's auc: 0.896693\n",
      "[11000]\ttraining's auc: 0.955657\tvalid_1's auc: 0.896705\n",
      "[12000]\ttraining's auc: 0.95889\tvalid_1's auc: 0.896625\n",
      "[13000]\ttraining's auc: 0.96201\tvalid_1's auc: 0.896732\n",
      "[14000]\ttraining's auc: 0.964954\tvalid_1's auc: 0.896575\n",
      "[15000]\ttraining's auc: 0.967715\tvalid_1's auc: 0.896432\n",
      "[16000]\ttraining's auc: 0.97034\tvalid_1's auc: 0.896166\n",
      "Early stopping, best iteration is:\n",
      "[13131]\ttraining's auc: 0.96242\tvalid_1's auc: 0.896804\n",
      "\n",
      "AUC = 0.8968036051139934\n",
      "\n",
      "Best Model:2 Best Score:0.8972817112442295\n",
      "\n",
      "kmeans:5\n",
      "\n",
      "pred_num=19716\n",
      "other_num=180284\n",
      "\n",
      "\n",
      "sample_num=162255\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901447\tvalid_1's auc: 0.877715\n",
      "[2000]\ttraining's auc: 0.913237\tvalid_1's auc: 0.885677\n",
      "[3000]\ttraining's auc: 0.921143\tvalid_1's auc: 0.889927\n",
      "[4000]\ttraining's auc: 0.927091\tvalid_1's auc: 0.892692\n",
      "[5000]\ttraining's auc: 0.932248\tvalid_1's auc: 0.894148\n",
      "[6000]\ttraining's auc: 0.936692\tvalid_1's auc: 0.894692\n",
      "[7000]\ttraining's auc: 0.94079\tvalid_1's auc: 0.894771\n",
      "[8000]\ttraining's auc: 0.944719\tvalid_1's auc: 0.894627\n",
      "[9000]\ttraining's auc: 0.948454\tvalid_1's auc: 0.894804\n",
      "Early stopping, best iteration is:\n",
      "[6716]\ttraining's auc: 0.939662\tvalid_1's auc: 0.894904\n",
      "\n",
      "AUC = 0.8949037759482342\n",
      "\n",
      "sample_num=162255\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901808\tvalid_1's auc: 0.876437\n",
      "[2000]\ttraining's auc: 0.913524\tvalid_1's auc: 0.884717\n",
      "[3000]\ttraining's auc: 0.921376\tvalid_1's auc: 0.889015\n",
      "[4000]\ttraining's auc: 0.927318\tvalid_1's auc: 0.891256\n",
      "[5000]\ttraining's auc: 0.932459\tvalid_1's auc: 0.892691\n",
      "[6000]\ttraining's auc: 0.936914\tvalid_1's auc: 0.893267\n",
      "[7000]\ttraining's auc: 0.941062\tvalid_1's auc: 0.893709\n",
      "[8000]\ttraining's auc: 0.94497\tvalid_1's auc: 0.89409\n",
      "[9000]\ttraining's auc: 0.948642\tvalid_1's auc: 0.893931\n",
      "[10000]\ttraining's auc: 0.952133\tvalid_1's auc: 0.893862\n",
      "Early stopping, best iteration is:\n",
      "[7881]\ttraining's auc: 0.944518\tvalid_1's auc: 0.894103\n",
      "\n",
      "AUC = 0.8941034743292346\n",
      "\n",
      "sample_num=162255\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901189\tvalid_1's auc: 0.876937\n",
      "[2000]\ttraining's auc: 0.912949\tvalid_1's auc: 0.885077\n",
      "[3000]\ttraining's auc: 0.920879\tvalid_1's auc: 0.889071\n",
      "[4000]\ttraining's auc: 0.926987\tvalid_1's auc: 0.891477\n",
      "[5000]\ttraining's auc: 0.932066\tvalid_1's auc: 0.892762\n",
      "[6000]\ttraining's auc: 0.936653\tvalid_1's auc: 0.89335\n",
      "[7000]\ttraining's auc: 0.940767\tvalid_1's auc: 0.893725\n",
      "[8000]\ttraining's auc: 0.944611\tvalid_1's auc: 0.894173\n",
      "[9000]\ttraining's auc: 0.948319\tvalid_1's auc: 0.893859\n",
      "[10000]\ttraining's auc: 0.951795\tvalid_1's auc: 0.89405\n",
      "Early stopping, best iteration is:\n",
      "[7958]\ttraining's auc: 0.944461\tvalid_1's auc: 0.894215\n",
      "\n",
      "AUC = 0.8942153907855473\n",
      "\n",
      "Best Model:2 Best Score:0.8949037759482342\n",
      "\n",
      "kmeans:6\n",
      "\n",
      "pred_num=20415\n",
      "other_num=179585\n",
      "\n",
      "\n",
      "sample_num=161626\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900558\tvalid_1's auc: 0.886966\n",
      "[2000]\ttraining's auc: 0.912385\tvalid_1's auc: 0.89425\n",
      "[3000]\ttraining's auc: 0.920101\tvalid_1's auc: 0.898709\n",
      "[4000]\ttraining's auc: 0.926147\tvalid_1's auc: 0.901122\n",
      "[5000]\ttraining's auc: 0.931266\tvalid_1's auc: 0.902204\n",
      "[6000]\ttraining's auc: 0.935846\tvalid_1's auc: 0.90261\n",
      "[7000]\ttraining's auc: 0.94009\tvalid_1's auc: 0.903018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000]\ttraining's auc: 0.943982\tvalid_1's auc: 0.902761\n",
      "[9000]\ttraining's auc: 0.947668\tvalid_1's auc: 0.90263\n",
      "[10000]\ttraining's auc: 0.951231\tvalid_1's auc: 0.902635\n",
      "Early stopping, best iteration is:\n",
      "[7061]\ttraining's auc: 0.940358\tvalid_1's auc: 0.903038\n",
      "\n",
      "AUC = 0.9030379076557511\n",
      "\n",
      "sample_num=161626\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901218\tvalid_1's auc: 0.886768\n",
      "[2000]\ttraining's auc: 0.913131\tvalid_1's auc: 0.893927\n",
      "[3000]\ttraining's auc: 0.92083\tvalid_1's auc: 0.898169\n",
      "[4000]\ttraining's auc: 0.926844\tvalid_1's auc: 0.900356\n",
      "[5000]\ttraining's auc: 0.931901\tvalid_1's auc: 0.901485\n",
      "[6000]\ttraining's auc: 0.936341\tvalid_1's auc: 0.901986\n",
      "[7000]\ttraining's auc: 0.940516\tvalid_1's auc: 0.902425\n",
      "[8000]\ttraining's auc: 0.944388\tvalid_1's auc: 0.902703\n",
      "[9000]\ttraining's auc: 0.948107\tvalid_1's auc: 0.902447\n",
      "[10000]\ttraining's auc: 0.951606\tvalid_1's auc: 0.902597\n",
      "[11000]\ttraining's auc: 0.955017\tvalid_1's auc: 0.902517\n",
      "Early stopping, best iteration is:\n",
      "[8001]\ttraining's auc: 0.944392\tvalid_1's auc: 0.902707\n",
      "\n",
      "AUC = 0.9027071496111587\n",
      "\n",
      "sample_num=161626\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900503\tvalid_1's auc: 0.886598\n",
      "[2000]\ttraining's auc: 0.912453\tvalid_1's auc: 0.894329\n",
      "[3000]\ttraining's auc: 0.920269\tvalid_1's auc: 0.898546\n",
      "[4000]\ttraining's auc: 0.926343\tvalid_1's auc: 0.900879\n",
      "[5000]\ttraining's auc: 0.931495\tvalid_1's auc: 0.902112\n",
      "[6000]\ttraining's auc: 0.935969\tvalid_1's auc: 0.902808\n",
      "[7000]\ttraining's auc: 0.940208\tvalid_1's auc: 0.902784\n",
      "[8000]\ttraining's auc: 0.944099\tvalid_1's auc: 0.903007\n",
      "[9000]\ttraining's auc: 0.947857\tvalid_1's auc: 0.903067\n",
      "[10000]\ttraining's auc: 0.951406\tvalid_1's auc: 0.903078\n",
      "[11000]\ttraining's auc: 0.954802\tvalid_1's auc: 0.902874\n",
      "[12000]\ttraining's auc: 0.958044\tvalid_1's auc: 0.902429\n",
      "Early stopping, best iteration is:\n",
      "[9880]\ttraining's auc: 0.951008\tvalid_1's auc: 0.90316\n",
      "\n",
      "AUC = 0.9031601430422379\n",
      "\n",
      "Best Model:2 Best Score:0.9031601430422379\n",
      "\n",
      "kmeans:7\n",
      "\n",
      "pred_num=20884\n",
      "other_num=179116\n",
      "\n",
      "\n",
      "sample_num=161204\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901057\tvalid_1's auc: 0.890477\n",
      "[2000]\ttraining's auc: 0.912963\tvalid_1's auc: 0.898098\n",
      "[3000]\ttraining's auc: 0.920898\tvalid_1's auc: 0.902238\n",
      "[4000]\ttraining's auc: 0.926815\tvalid_1's auc: 0.903757\n",
      "[5000]\ttraining's auc: 0.93199\tvalid_1's auc: 0.904873\n",
      "[6000]\ttraining's auc: 0.936487\tvalid_1's auc: 0.905386\n",
      "[7000]\ttraining's auc: 0.940648\tvalid_1's auc: 0.905703\n",
      "[8000]\ttraining's auc: 0.944532\tvalid_1's auc: 0.905726\n",
      "[9000]\ttraining's auc: 0.948263\tvalid_1's auc: 0.905759\n",
      "[10000]\ttraining's auc: 0.951828\tvalid_1's auc: 0.905566\n",
      "[11000]\ttraining's auc: 0.955168\tvalid_1's auc: 0.905419\n",
      "Early stopping, best iteration is:\n",
      "[8315]\ttraining's auc: 0.945734\tvalid_1's auc: 0.905843\n",
      "\n",
      "AUC = 0.9058425513921577\n",
      "\n",
      "sample_num=161204\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.90012\tvalid_1's auc: 0.889782\n",
      "[2000]\ttraining's auc: 0.912131\tvalid_1's auc: 0.898095\n",
      "[3000]\ttraining's auc: 0.920167\tvalid_1's auc: 0.901928\n",
      "[4000]\ttraining's auc: 0.926256\tvalid_1's auc: 0.903762\n",
      "[5000]\ttraining's auc: 0.931471\tvalid_1's auc: 0.905091\n",
      "[6000]\ttraining's auc: 0.935957\tvalid_1's auc: 0.905586\n",
      "[7000]\ttraining's auc: 0.940182\tvalid_1's auc: 0.905752\n",
      "[8000]\ttraining's auc: 0.944088\tvalid_1's auc: 0.905866\n",
      "[9000]\ttraining's auc: 0.947912\tvalid_1's auc: 0.905854\n",
      "[10000]\ttraining's auc: 0.951491\tvalid_1's auc: 0.905806\n",
      "[11000]\ttraining's auc: 0.954876\tvalid_1's auc: 0.905731\n",
      "Early stopping, best iteration is:\n",
      "[8860]\ttraining's auc: 0.947394\tvalid_1's auc: 0.905941\n",
      "\n",
      "AUC = 0.9059410641870299\n",
      "\n",
      "sample_num=161204\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901355\tvalid_1's auc: 0.889344\n",
      "[2000]\ttraining's auc: 0.913082\tvalid_1's auc: 0.897564\n",
      "[3000]\ttraining's auc: 0.921006\tvalid_1's auc: 0.900944\n",
      "[4000]\ttraining's auc: 0.927044\tvalid_1's auc: 0.903122\n",
      "[5000]\ttraining's auc: 0.932296\tvalid_1's auc: 0.904284\n",
      "[6000]\ttraining's auc: 0.936832\tvalid_1's auc: 0.904887\n",
      "[7000]\ttraining's auc: 0.941003\tvalid_1's auc: 0.905186\n",
      "[8000]\ttraining's auc: 0.944871\tvalid_1's auc: 0.905237\n",
      "[9000]\ttraining's auc: 0.948613\tvalid_1's auc: 0.905183\n",
      "[10000]\ttraining's auc: 0.952162\tvalid_1's auc: 0.904969\n",
      "[11000]\ttraining's auc: 0.955578\tvalid_1's auc: 0.904807\n",
      "Early stopping, best iteration is:\n",
      "[8671]\ttraining's auc: 0.947419\tvalid_1's auc: 0.90531\n",
      "\n",
      "AUC = 0.905309556990871\n",
      "\n",
      "Best Model:2 Best Score:0.9059410641870299\n",
      "\n",
      "kmeans:8\n",
      "\n",
      "pred_num=18861\n",
      "other_num=181139\n",
      "\n",
      "\n",
      "sample_num=163025\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901969\tvalid_1's auc: 0.876037\n",
      "[2000]\ttraining's auc: 0.913789\tvalid_1's auc: 0.884709\n",
      "[3000]\ttraining's auc: 0.921476\tvalid_1's auc: 0.888739\n",
      "[4000]\ttraining's auc: 0.927602\tvalid_1's auc: 0.891154\n",
      "[5000]\ttraining's auc: 0.93271\tvalid_1's auc: 0.89284\n",
      "[6000]\ttraining's auc: 0.937205\tvalid_1's auc: 0.893379\n",
      "[7000]\ttraining's auc: 0.941323\tvalid_1's auc: 0.893713\n",
      "[8000]\ttraining's auc: 0.945248\tvalid_1's auc: 0.893912\n",
      "[9000]\ttraining's auc: 0.948895\tvalid_1's auc: 0.893975\n",
      "[10000]\ttraining's auc: 0.952393\tvalid_1's auc: 0.894201\n",
      "[11000]\ttraining's auc: 0.95575\tvalid_1's auc: 0.894207\n",
      "[12000]\ttraining's auc: 0.958932\tvalid_1's auc: 0.894316\n",
      "[13000]\ttraining's auc: 0.961996\tvalid_1's auc: 0.89415\n",
      "[14000]\ttraining's auc: 0.96486\tvalid_1's auc: 0.894053\n",
      "Early stopping, best iteration is:\n",
      "[11864]\ttraining's auc: 0.9585\tvalid_1's auc: 0.894361\n",
      "\n",
      "AUC = 0.8943613599929062\n",
      "\n",
      "sample_num=163025\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.902333\tvalid_1's auc: 0.87582\n",
      "[2000]\ttraining's auc: 0.913998\tvalid_1's auc: 0.884417\n",
      "[3000]\ttraining's auc: 0.921871\tvalid_1's auc: 0.888437\n",
      "[4000]\ttraining's auc: 0.927727\tvalid_1's auc: 0.890707\n",
      "[5000]\ttraining's auc: 0.932739\tvalid_1's auc: 0.892094\n",
      "[6000]\ttraining's auc: 0.937201\tvalid_1's auc: 0.892713\n",
      "[7000]\ttraining's auc: 0.941301\tvalid_1's auc: 0.892907\n",
      "[8000]\ttraining's auc: 0.945142\tvalid_1's auc: 0.892959\n",
      "[9000]\ttraining's auc: 0.948772\tvalid_1's auc: 0.892927\n",
      "[10000]\ttraining's auc: 0.952279\tvalid_1's auc: 0.892957\n",
      "[11000]\ttraining's auc: 0.955609\tvalid_1's auc: 0.892941\n",
      "[12000]\ttraining's auc: 0.958794\tvalid_1's auc: 0.892813\n",
      "Early stopping, best iteration is:\n",
      "[9796]\ttraining's auc: 0.95157\tvalid_1's auc: 0.893059\n",
      "\n",
      "AUC = 0.8930593908038681\n",
      "\n",
      "sample_num=163025\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.902391\tvalid_1's auc: 0.876662\n",
      "[2000]\ttraining's auc: 0.91407\tvalid_1's auc: 0.884767\n",
      "[3000]\ttraining's auc: 0.92175\tvalid_1's auc: 0.888512\n",
      "[4000]\ttraining's auc: 0.927735\tvalid_1's auc: 0.890936\n",
      "[5000]\ttraining's auc: 0.932857\tvalid_1's auc: 0.892404\n",
      "[6000]\ttraining's auc: 0.937298\tvalid_1's auc: 0.893224\n",
      "[7000]\ttraining's auc: 0.941364\tvalid_1's auc: 0.893611\n",
      "[8000]\ttraining's auc: 0.945203\tvalid_1's auc: 0.893746\n",
      "[9000]\ttraining's auc: 0.948891\tvalid_1's auc: 0.893763\n",
      "[10000]\ttraining's auc: 0.952361\tvalid_1's auc: 0.893779\n",
      "[11000]\ttraining's auc: 0.955714\tvalid_1's auc: 0.893758\n",
      "[12000]\ttraining's auc: 0.958894\tvalid_1's auc: 0.893695\n",
      "[13000]\ttraining's auc: 0.961934\tvalid_1's auc: 0.89359\n",
      "Early stopping, best iteration is:\n",
      "[10802]\ttraining's auc: 0.955063\tvalid_1's auc: 0.893844\n",
      "\n",
      "AUC = 0.8938442312323477\n",
      "\n",
      "Best Model:2 Best Score:0.8943613599929062\n",
      "\n",
      "kmeans:9\n",
      "\n",
      "pred_num=20155\n",
      "other_num=179845\n",
      "\n",
      "\n",
      "sample_num=161860\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.901253\tvalid_1's auc: 0.885669\n",
      "[2000]\ttraining's auc: 0.913415\tvalid_1's auc: 0.894786\n",
      "[3000]\ttraining's auc: 0.921176\tvalid_1's auc: 0.898453\n",
      "[4000]\ttraining's auc: 0.927202\tvalid_1's auc: 0.900414\n",
      "[5000]\ttraining's auc: 0.932323\tvalid_1's auc: 0.901375\n",
      "[6000]\ttraining's auc: 0.9368\tvalid_1's auc: 0.901971\n",
      "[7000]\ttraining's auc: 0.940937\tvalid_1's auc: 0.9024\n",
      "[8000]\ttraining's auc: 0.944794\tvalid_1's auc: 0.902568\n",
      "[9000]\ttraining's auc: 0.948499\tvalid_1's auc: 0.9025\n",
      "[10000]\ttraining's auc: 0.952088\tvalid_1's auc: 0.902512\n",
      "[11000]\ttraining's auc: 0.955415\tvalid_1's auc: 0.902483\n",
      "Early stopping, best iteration is:\n",
      "[8696]\ttraining's auc: 0.947371\tvalid_1's auc: 0.902609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC = 0.9026087004856823\n",
      "\n",
      "sample_num=161860\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900799\tvalid_1's auc: 0.887982\n",
      "[2000]\ttraining's auc: 0.912877\tvalid_1's auc: 0.896398\n",
      "[3000]\ttraining's auc: 0.920928\tvalid_1's auc: 0.899678\n",
      "[4000]\ttraining's auc: 0.926999\tvalid_1's auc: 0.901534\n",
      "[5000]\ttraining's auc: 0.932262\tvalid_1's auc: 0.90246\n",
      "[6000]\ttraining's auc: 0.936843\tvalid_1's auc: 0.902734\n",
      "[7000]\ttraining's auc: 0.940976\tvalid_1's auc: 0.903141\n",
      "[8000]\ttraining's auc: 0.944898\tvalid_1's auc: 0.903214\n",
      "[9000]\ttraining's auc: 0.948632\tvalid_1's auc: 0.90312\n",
      "[10000]\ttraining's auc: 0.952171\tvalid_1's auc: 0.903191\n",
      "[11000]\ttraining's auc: 0.955465\tvalid_1's auc: 0.903218\n",
      "[12000]\ttraining's auc: 0.958693\tvalid_1's auc: 0.903065\n",
      "[13000]\ttraining's auc: 0.961777\tvalid_1's auc: 0.902919\n",
      "Early stopping, best iteration is:\n",
      "[10725]\ttraining's auc: 0.954562\tvalid_1's auc: 0.903338\n",
      "\n",
      "AUC = 0.9033382314356835\n",
      "\n",
      "sample_num=161860\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900384\tvalid_1's auc: 0.887577\n",
      "[2000]\ttraining's auc: 0.912451\tvalid_1's auc: 0.895148\n",
      "[3000]\ttraining's auc: 0.920447\tvalid_1's auc: 0.898836\n",
      "[4000]\ttraining's auc: 0.926539\tvalid_1's auc: 0.900919\n",
      "[5000]\ttraining's auc: 0.931831\tvalid_1's auc: 0.901807\n",
      "[6000]\ttraining's auc: 0.936378\tvalid_1's auc: 0.902297\n",
      "[7000]\ttraining's auc: 0.940551\tvalid_1's auc: 0.902404\n",
      "[8000]\ttraining's auc: 0.944547\tvalid_1's auc: 0.902615\n",
      "[9000]\ttraining's auc: 0.948213\tvalid_1's auc: 0.902731\n",
      "[10000]\ttraining's auc: 0.95182\tvalid_1's auc: 0.902647\n",
      "[11000]\ttraining's auc: 0.955175\tvalid_1's auc: 0.90259\n",
      "[12000]\ttraining's auc: 0.958434\tvalid_1's auc: 0.902543\n",
      "Early stopping, best iteration is:\n",
      "[9309]\ttraining's auc: 0.949337\tvalid_1's auc: 0.90281\n",
      "\n",
      "AUC = 0.9028104646042401\n",
      "\n",
      "Best Model:2 Best Score:0.9033382314356835\n",
      "Over All AUC = 0.9004076708987718\n"
     ]
    }
   ],
   "source": [
    "y_test = np.zeros((len(x_test), 1))\n",
    "y_train_preds = np.zeros((len(x_train), 1))\n",
    "for i in range(0,10):\n",
    "    print(\"\")\n",
    "    print(\"kmeans:\"+str(i))\n",
    "    x_train['target']=y_train\n",
    "    pred_x=x_train.query('kmeans =='+str(i))\n",
    "    other_x=x_train.query('not kmeans =='+str(i))\n",
    "    pred_y=pred_x['target']\n",
    "    other_y=other_x['target']\n",
    "    pred_x=pred_x.drop('target',axis=1)\n",
    "    other_x=other_x.drop('target',axis=1)\n",
    "    \n",
    "    best_model=best_clf(pred_x,pred_y,other_x,other_y)\n",
    "    \n",
    "    x_test_pred=x_test[x_test['kmeans']==i]\n",
    "    \n",
    "    y_test_pred=best_model.predict(x_test_pred)\n",
    "    y_train_pred=best_model.predict(pred_x)\n",
    "    \n",
    "    x_test_pred_index=x_test_pred.index\n",
    "    x_train_pred_index=pred_x.index\n",
    "    \n",
    "    y_test[x_test_pred_index, :]=y_test_pred.reshape((-1, 1))\n",
    "    y_train_preds[x_train_pred_index, :]=y_train_pred.reshape((-1, 1))\n",
    "\n",
    "print(\"Over All AUC = {}\".format(roc_auc_score(y_train, y_train_preds)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08171734],\n",
       "       [0.21198915],\n",
       "       [0.21217316],\n",
       "       ...,\n",
       "       [0.00286547],\n",
       "       [0.06510396],\n",
       "       [0.06218417]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果保存\n",
    "if (submit_flg ==1 and sampling_flg==0):\n",
    "    sample = pd.read_csv(sample_submission_dir)\n",
    "    sample.target = y_test\n",
    "    sample.ID_code = test_df['ID_code']\n",
    "    sample.to_csv(test_preds_dir, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
